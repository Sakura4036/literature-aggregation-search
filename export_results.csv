title,authors,publication_year,venue,doi,pmid,arxiv_id,abstract,citation_count,source
Influence maximization in social networks using graph embedding and graph neural network,Sanjay Kumar (N/A); Abhishek Mallik (N/A); Anavi Khetarpal (N/A); B. S. Panda (N/A),2022,Information Sciences,10.1016/j.ins.2022.06.075,,,,139,semantic_scholar
Balancing Efficiency and Expressiveness: Subgraph GNNs with Walk-Based Centrality,Joshua Southern (N/A); Yam Eitan (N/A); Guy Bar-Shalom (N/A); Michael M. Bronstein (N/A); Haggai Maron (N/A); Fabrizio Frasca (N/A),2025,arXiv.org,10.48550/arXiv.2501.03113,,2501.03113,"Subgraph GNNs have emerged as promising architectures that overcome the expressiveness limitations of Graph Neural Networks (GNNs) by processing bags of subgraphs. Despite their compelling empirical performance, these methods are afflicted by a high computational complexity: they process bags whose size grows linearly in the number of nodes, hindering their applicability to larger graphs. In this work, we propose an effective and easy-to-implement approach to dramatically alleviate the computational cost of Subgraph GNNs and unleash broader applications thereof. Our method, dubbed HyMN, leverages walk-based centrality measures to sample a small number of relevant subgraphs and drastically reduce the bag size. By drawing a connection to perturbation analysis, we highlight the strength of the proposed centrality-based subgraph sampling, and further prove that these walk-based centralities can be additionally used as Structural Encodings for improved discriminative power. A comprehensive set of experimental results demonstrates that HyMN provides an effective synthesis of expressiveness, efficiency, and downstream performance, unlocking the application of Subgraph GNNs to dramatically larger graphs. Not only does our method outperform more sophisticated subgraph sampling approaches, it is also competitive, and sometimes better, than other state-of-the-art approaches for a fraction of their runtime.",1,semantic_scholar
A New Wavelet-Based Algorithm for Compression of Emg Signals,Pedro de A Berger (N/A); F. de O. Nascimento (N/A); A. F. da Rocha (N/A); Joao L. A. Carvalho (N/A),2007,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,10.1109/IEMBS.2007.4352600,18002266,,,18,semantic_scholar
Convolutional Neural Network Techniques on X-ray Images for Covid-19 Classification,Eugenio Vocaturo (N/A); E. Zumpano (N/A); Luciano Caroprese (N/A),2021,IEEE International Conference on Bioinformatics and Biomedicine,10.1109/BIBM52615.2021.9669784,,,"At the end of 2019, the World Health Organization (WHO) referred that the Public Health Commission of Hubei Province, China, reported cases of severe and unknown pneumonia. A new coronavirus, SARS-CoV-2, was identified as responsible for the lung infection, called COVID-19 (coronavirus disease 2019). An early diagnosis of those carrying the virus becomes crucial to contain the spread, morbidity and mortality of the pandemic. The definitive diagnosis is made through specific tests, among which imaging tests play a very important role. Achieving this goal cannot be separated from radiological examination, and chest X-ray is the most easily available and least expensive alternative. The use of X-ray chest radiographs, as an element that assists the diagnosis and that allows the follow up of the disease, is the subject of many publications that adopt machine learning approaches. This work focuses on the most adopted Convolutional Neural Network Techniques applied on chest X-ray images.",9,semantic_scholar
End-to-End Representation Learning for Correlation Filter Based Tracking,Jack Valmadre (N/A); Luca Bertinetto (N/A); João F. Henriques (N/A); A. Vedaldi (N/A); Philip H. S. Torr (N/A),2017,Computer Vision and Pattern Recognition,10.1109/CVPR.2017.531,,1704.06036,"The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.",1406,semantic_scholar
Convolutional Nonlinear Differential Recurrent Neural Networks for Crowd Scene Understanding,Naifan Zhuang (N/A); T. Kieu (N/A); Jun Ye (N/A); K. Hua (N/A),2018,International Journal of Semantic Computing,10.1142/S1793351X18400196,,,"With the growth of crowd phenomena in the real world, crowd scene understanding is becoming an important task in anomaly detection and public security. Visual ambiguities and occlusions, high density, low mobility, and scene semantics, however, make this problem a great challenge. In this paper, we propose an end-to-end deep architecture, convolutional nonlinear differential recurrent neural networks (CNDRNNs), for crowd scene understanding. CNDRNNs consist of GoogleNet Inception V3 convolutional neural networks (CNNs) and nonlinear differential recurrent neural networks (RNNs). Different from traditional non-end-to-end solutions which separate the steps of feature extraction and parameter learning, CNDRNN utilizes a unified deep model to optimize the parameters of CNN and RNN hand in hand. It thus has the potential of generating a more harmonious model. The proposed architecture takes sequential raw image data as input, and does not rely on tracklet or trajectory detection. It thus has clear advantages over the traditional flow-based and trajectory-based methods, especially in challenging crowd scenarios of high density and low mobility. Taking advantage of CNN and RNN, CNDRNN can effectively analyze the crowd semantics. Specifically, CNN is good at modeling the semantic crowd scene information. On the other hand, nonlinear differential RNN models the motion information. The individual and increasing orders of derivative of states (DoS) in differential RNN can progressively build up the ability of the long short-term memory (LSTM) gates to detect different levels of salient dynamical patterns in deeper stacked layers modeling higher orders of DoS. Lastly, existing LSTM-based crowd scene solutions explore deep temporal information and are claimed to be “deep in time.” Our proposed method CNDRNN, however, models the spatial and temporal information in a unified architecture and achieves “deep in space and time.” Extensive performance studies on the Violent-Flows, CUHK Crowd, and NUS-HGA datasets show that the proposed technique significantly outperforms state-of-the-art methods.",3,semantic_scholar
Neural parameters estimation for brain tumor growth modeling,I. Ezhov (N/A); Jana Lipková (N/A); Suprosanna Shit (N/A); F. Kofler (N/A); Nora Collomb (N/A); B. Lemasson (N/A); Emmanuel L Barbier (N/A); Bjoern H Menze (N/A),2019,International Conference on Medical Image Computing and Computer-Assisted Intervention,10.1007/978-3-030-32245-8_87,,1907.00973,"Understanding the dynamics of brain tumor progression is essential for optimal treatment planning. Cast in a mathematical formulation, it is typically viewed as evaluation of a system of partial differential equations, wherein the physiological processes that govern the growth of the tumor are considered. To personalize the model, i.e. find a relevant set of parameters, with respect to the tumor dynamics of a particular patient, the model is informed from empirical data, e.g., medical images obtained from diagnostic modalities, such as magnetic-resonance imaging. Existing model-observation coupling schemes require a large number of forward integrations of the biophysical model and rely on simplifying assumption on the functional form, linking the output of the model with the image information. In this work, we propose a learning-based technique for the estimation of tumor growth model parameters from medical scans. The technique allows for explicit evaluation of the posterior distribution of the parameters by sequentially training a mixture-density network, relaxing the constraint on the functional form and reducing the number of samples necessary to propagate through the forward model for the estimation. We test the method on synthetic and real scans of rats injected with brain tumors to calibrate the model and to predict tumor progression.",24,semantic_scholar
Multi-Branch Network for Color Image Denoising Using Dilated Convolution and Attention Mechanisms,Minh-Thien Duong (N/A); Bao-Tran Nguyen Thi (N/A); Seongsoo Lee (N/A); Min-Cheol Hong (N/A),2024,Italian National Conference on Sensors,10.3390/s24113608,38894398,,"Image denoising is regarded as an ill-posed problem in computer vision tasks that removes additive noise from imaging sensors. Recently, several convolution neural network-based image-denoising methods have achieved remarkable advances. However, it is difficult for a simple denoising network to recover aesthetically pleasing images owing to the complexity of image content. Therefore, this study proposes a multi-branch network to improve the performance of the denoising method. First, the proposed network is designed based on a conventional autoencoder to learn multi-level contextual features from input images. Subsequently, we integrate two modules into the network, including the Pyramid Context Module (PCM) and the Residual Bottleneck Attention Module (RBAM), to extract salient information for the training process. More specifically, PCM is applied at the beginning of the network to enlarge the receptive field and successfully address the loss of global information using dilated convolution. Meanwhile, RBAM is inserted into the middle of the encoder and decoder to eliminate degraded features and reduce undesired artifacts. Finally, extensive experimental results prove the superiority of the proposed method over state-of-the-art deep-learning methods in terms of objective and subjective performances.",6,semantic_scholar
Analysis of chaotic dynamical systems with autoencoders,N. Almazova (N/A); G. D. Barmparis (N/A); G. P. Tsironis (N/A),2021,Chaos,10.1063/5.0055673,34717343,2109.13078,"We focus on chaotic dynamical systems and analyze their time series with the use of autoencoders, i.e., configurations of neural networks that map identical output to input. This analysis results in the determination of the latent space dimension of each system and thus determines the minimal number of nodes necessary to capture the essential information contained in the chaotic time series. The constructed chaotic autoencoders generate similar maximal Lyapunov exponents as the original chaotic systems and thus encompass their essential dynamical information.",7,semantic_scholar
Neuroanniversary 2019,P. Eling (N/A),2019,Journal of the History of the Neurosciences,10.1080/0964704X.2018.1561070,30633648,,"Josef Gerstmann (1887–1969) graduated at the Medical University in Vienna in 1912. He worked there with Julius Wagner von Jauregg (1857–1940), lectured in neurology and psychiatry, and became a professor in 1930. He emigrated to the United States in 1938, working in Ohio, Washington, and New York, where he died in 1969. He published his best-known paper on finger-agnosia in 1924. The Gerstmann syndrome is characterized by finger agnosia, dysgraphia, dyscalculia, and left-right syndrome and has been associated with damage to the inferior parietal lobe of the dominant hemisphere. American neurophysiologist Warren Sturgish McCullough (1898–1969) and logician and computer scientist Walter Pitts (1923–1969) both passed away in 1969. McCulloch and Pitts created computational models based on mathematical algorithms called threshold logic, which split the inquiry into two distinct approaches, one focused on biological processes in the brain (single cell recording) and the other on the application of neural networks to artificial intelligence. Johannes Maagaard Nielsen (1890–1969), born in Aarslev, Denmark, moved to the United States in 1896. He became clinical professor of neurology at the University of California at Los Angeles. He was one of the founders of the Society of Biological Psychiatry and was its president from 1947 to 1948. In 1936, he published Agnosia, Apraxia, Aphasia: Their Value in Cerebral Localization, a textbook for neurologists. The Society for Neuroscience was founded in 1969 by Ralph W. Gerard (1900–1974), an American neurophysiologist and behavioral scientist. The first annual meeting of the society was held in Washington, D.C., in 1971, and it was attended by 1,396 scientists. The Society publishes the Journal of Neuroscience. In 1969, Suzuki Jiro and Takaku Akira published their study on “cerebrovascular moyamoya disease.” The disease produces an abnormal net-like blood vessel picture, characterized with the Japanese word moyamoya—an expression for something hazy like a puff of cigarette smoke drifting in the air. That same year, a case report of a patient with a selective impairment of auditory verbal short-term memory was presented in Brain by British neuropsychologists Elizabeth Warrington and Tim Shallice. This case triggered a completely different view of the role of short-term memory, now becoming a working memory.",0,semantic_scholar
Computational Complexity of Learning Neural Networks: Smoothness and Degeneracy,Amit Daniely (N/A); N. Srebro (N/A); Gal Vardi (N/A),2023,Neural Information Processing Systems,,,2302.07426,"Understanding when neural networks can be learned efficiently is a fundamental question in learning theory. Existing hardness results suggest that assumptions on both the input distribution and the network's weights are necessary for obtaining efficient algorithms. Moreover, it was previously shown that depth-$2$ networks can be efficiently learned under the assumptions that the input distribution is Gaussian, and the weight matrix is non-degenerate. In this work, we study whether such assumptions may suffice for learning deeper networks and prove negative results. We show that learning depth-$3$ ReLU networks under the Gaussian input distribution is hard even in the smoothed-analysis framework, where a random noise is added to the network's parameters. It implies that learning depth-$3$ ReLU networks under the Gaussian distribution is hard even if the weight matrices are non-degenerate. Moreover, we consider depth-$2$ networks, and show hardness of learning in the smoothed-analysis framework, where both the network parameters and the input distribution are smoothed. Our hardness results are under a well-studied assumption on the existence of local pseudorandom generators.",5,semantic_scholar
Hybrid CNN-Transformer For Marine Aquaculture Semantic Segmentation Based on Polsar Images,Keyuan Liu (N/A); Danchen Zheng (N/A); Jianchao Fan (N/A),2024,IEEE International Geoscience and Remote Sensing Symposium,10.1109/IGARSS53475.2024.10642091,,,"Marine floating raft aquaculture is an important development of marine natural resources, therefore it is necessary to plan the floating raft aquaculture area reasonably. Polarimetric synthetic aperture radar (PolSAR) can record polarization information of scattered echoes from floating rafts and has become an important means of monitoring floating raft aquaculture areas. At present, mainstream semantic segmentation architectures include convolutional neural networks (CNN) and Transformers. There are many methods that can be used for the segmentation task of PolSAR floating rafts, but they are difficult to combine both local and global features of PolSAR data simultaneously. Therefore, this paper introduces a method that integrates CSwin Transformer and ResNet into a segmentation network (CRSegNet). It effectively extracts global and local information of aquaculture rafts, not only grasping the relative position relationship between background and rafts, but also smoothly segmenting rafts from each other. Therefore, it achieved better results than mainstream models.",0,semantic_scholar
Neural Network-Based Human Motion Predictor and Smoother,Stella Grasshof (N/A); Mathias Bastholm (N/A); Sami S. Brandt (N/A),2023,SN Computer Science,10.1007/s42979-023-02195-0,,,"Though continuous advances in the field of human pose estimation, it remains a challenge to retrieve high-quality recordings from real-life human motion using commodity hardware. Therefore, this work focuses on predicting and improving estimates for human motion with the aim of achieving production quality for skinned mesh animations by off-the-shelf webcams. We take advantage of recent findings in the field by employing a recurrent neural network architecture to (1) predict and (2) denoise human motion, with the intention of bridging the gap between cheap recording methods and high-quality recording. First, we propose an LSTM to predict short-term human motion, which achieves competitive results to state-of-the-art methods. Then, we adapt this model architecture and train it to clean up noisy human motion from two 3D low-quality input sources, and hence mimic a real-world scenario of recording human motion which yields noisy estimates. Experiments on simulated data show that the model is capable of significantly reducing noise, and it opens the way for future work to test the model on annotated data.",1,semantic_scholar
Multi-objective Reinforcement Learning in Factored MDPs with Graph Neural Networks,M. Vincent (N/A); A. E. Seghrouchni (N/A); V. Corruble (N/A); Narayan Bernardin (N/A); Rami Kassab (N/A); F. Barbaresco (N/A),2023,Adaptive Agents and Multi-Agent Systems,10.5555/3545946.3599094,,,,0,semantic_scholar
Likelihood-free Bayesian analysis of neural network models,Brandon M. Turner (N/A); P. Sederberg (N/A); James L. McClelland (N/A),2013,BMC Neuroscience,10.1186/1471-2202-14-S1-P270,,,"The goal of cognitive modeling is to understand complex behaviors within a system of mathematically-specified mechanisms or processes; to assess the adequacy of the model to account for experimental data, and to obtain an estimate of the model parameters, which carry valuable information about how the model captures the observed behavior for both individuals and groups. From a theoretical perspective it is essential that we fully understand how the parameters of a model affect the model predictions, and those parameters interact with one another.

Despite the importance of understanding the full range of valid parameter estimates, difficulties encountered in deriving the full likelihood function have prevented the application of fully Bayesian analyses for many cognitive models, especially those that attempt to capture neurally-plausible mechanisms. Recent advances in likelihood-free techniques have allowed for new insights to simulation-based cognitive models [1-3]. Yet, present likelihood-free methods have two critical sources of error that continue to prevent their widespread adoption. The first source of error arises from the use of summary statistics that are not sufficient for the parameters of interest. When a set of summary statistics are not sufficient, one cannot guarantee convergence to the correct posterior distribution. Because it is impossible to guarantee that a summary statistic is sufficient when a likelihood function is unavailable, current likelihood-free estimation techniques introduce error in the posterior distribution, and this error is not directly measurable. The second source of error results from the tolerance threshold that is used to evaluate the approximate likelihood in some algorithms. Even when sufficient statistics are known, a nonzero tolerance threshold will result in inaccurate posterior estimates [2].

Here we present a new, fully-generalizable method, which we call the probability density approximation (PDA) method, for performing likelihood-free Bayesian parameter estimation that does not suffer from these sources of error. Our method works by generating a set of simulated data and constructing an estimate of the underlying probability density function through scaled kernel density estimation. We illustrate the importance of our method by comparing two neural network models of choice reaction time that have never been analyzed using Bayesian techniques due to their computational complexity: the Leaky Competing Accumulator (LCA) [4] model and the Feed-Forward Inhibition (FFI) [5] model. Both models embody neurologically plausible mechanisms such as ""leakage"", or the passive decay of evidence during a decision, and competition among alternative through either lateral inhibition (in the LCA model) or feed-forward inhibition (in the FFI model). However, it remains unclear as to which dynamical system best accounts for empirical data, due to the limitations imposed by intractable likelihoods. Specifically, complexity measures that take into account posterior uncertainty and model complexity have yet to be applied. Our method of Bayesian analysis leads to results favoring the competitive mechanisms in the LCA over the feed-forward inhibition in the FFI, and reveals parameter trade-offs within these neurologically plausible models as well as interesting individual differences.",0,semantic_scholar
Prompt-Driven Continual Graph Learning,Qi Wang (N/A); Tianfei Zhou (N/A); Ye Yuan (N/A); Rui Mao (N/A),2025,arXiv.org,10.48550/arXiv.2502.06327,,2502.06327,"Continual Graph Learning (CGL), which aims to accommodate new tasks over evolving graph data without forgetting prior knowledge, is garnering significant research interest. Mainstream solutions adopt the memory replay-based idea, ie, caching representative data from earlier tasks for retraining the graph model. However, this strategy struggles with scalability issues for constantly evolving graphs and raises concerns regarding data privacy. Inspired by recent advancements in the prompt-based learning paradigm, this paper introduces a novel prompt-driven continual graph learning (PROMPTCGL) framework, which learns a separate prompt for each incoming task and maintains the underlying graph neural network model fixed. In this way, PROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous tasks. More specifically, we propose hierarchical prompting to instruct the model from both feature- and topology-level to fully address the variability of task graphs in dynamic continual learning. Additionally, we develop a personalized prompt generator to generate tailored prompts for each graph node while minimizing the number of prompts needed, leading to constant memory consumption regardless of the graph scale. Extensive experiments on four benchmarks show that PROMPTCGL achieves superior performance against existing CGL approaches while significantly reducing memory consumption. Our code is available at https://github.com/QiWang98/PromptCGL.",0,semantic_scholar
Explicit Neural Network-Based Modeling of Time-Varying Circuits with a Single BJT in the Wave Digital Domain,Oliviero Massi (N/A); Shijie Yang (N/A); Riccardo Giampiccolo (N/A); Alberto Bernardini (N/A),2025,International Symposium on Circuits and Systems,10.1109/ISCAS56072.2025.11043816,,,"Time-varying circuits, either due to inherent physical phenomena or due to external user interactions, are ubiquitous, especially in audio processing gear. Examples of such circuits include amplification stages based on a single Bipolar Junction Transistor (BJT) with time-varying controls that adjust, for example, the transistor biasing point. In this manuscript, we extend a previously proposed Wave Digital (WD) methodology for the fully explicit simulation of circuits incorporating a single nonlinear two-port element to accommodate time-varying circuits. To this end, we introduce a novel definition for the explicit BJT WD model, using a nonlinear vector wave scattering equation implemented through a neural network architecture. The proposed approach is validated through the discrete-time simulation of a fuzz guitar pedal with two potentiometers, achieving accuracy levels comparable to standard circuit simulation software.",0,semantic_scholar
A vision-based zebra crossing detection method for people with visual impairments,Y. Akbari (N/A); Hanadi Hassen (N/A); Nandhini Subramanian (N/A); Jayakanth Kunhoth (N/A); S. Al-Maadeed (N/A); Wael K. M. Alhajyaseen (N/A),2020,"International Conference on Informatics, IoT, and Enabling Technologies",10.1109/ICIoT48696.2020.9089622,,,"Safe navigation for visually impaired is challenging without assistive technology. This paper proposes a pedestrian crossing detection approach to help visually impaired people. We introduce the use of multiple convolutional neural networks (CNNs) by utilizing wavelet transform subbands as inputs in which networks are trained to detect zebra crossing. In our method, the original image is decomposed into wavelet subbands, and the input images are constructed from image approximation based on the coefficients of three subbands. In the multiple networks approach, the segmentation results of the networks were integrated to create the final segmentation map. The results presented in this study prove that our method fully outperforms the SegNet networks and other state-of-the-art results using the Synthia database.",7,semantic_scholar
Evaluation of Backpropagation Neural Network Models for Early Prediction of Student’s Graduation in XYZ University,Ainul Yaqin (N/A); A. Laksito (N/A); S. Fatonah (N/A),2021,"International Journal on Advanced Science, Engineering and Information Technology",10.18517/IJASEIT.11.2.11152,,,"The study period of the student in a tertiary institution is undoubtedly essential in implementing the objectives of the tertiary institution, particularly for the implementation of the study program, so that its outcomes will affect accreditation. Prediction of students' study period can be a reference for higher education institutions in making policies for the future. Based on XYZ University data, especially in the informatics study program, many students have the different generation and concentration therein. In the implementation of students in studying, several factors, including the value of the Grade Point Average (GPA), can affect the study period taken. Likewise, the institutions often do not understand the conditions or predictive value of students' study period on campus. The application of neural networks in predicting the students’ study period at the XYZ University uses a network model with GPA values as input and 1 layer of hidden layers with 10, 50 and 100 neurons; learning rate values used are 0.01, 0.1 and 0.3 and 1 output target for the study period. Prediction results obtained the best results on the neuron network pattern 50 with 0.01 as a learning rate, which detail of MSE value, the training is 0,017516 and the testing is 0,047721, with an accuracy value of 77%.",9,semantic_scholar
Coordinating Human and Agent Behavior in Collective-Risk Scenarios,Elias Fernández Domingos (N/A); J. C. Burguillo (N/A); A. Nowé (N/A); T. Lenaerts (N/A),2017,AAAI Conference on Artificial Intelligence,10.1609/aaai.v31i1.11081,,,"

 Various social situations entail a collective risk. A well-known example is climate change, wherein the risk of a future environmental disaster clashes with the immediate economic interest of developed and developing countries. The collective-risk game operationalizes this kind of situations. The decision process of the participants is determined by how good they are in evaluating the probability of future risk as well as their ability to anticipate the actions of the opponents. Anticipatory behavior contrasts with the reactive theories often used to analyze social dilemmas. Our initial work can already show that anticipative agents are a better model to human behavior than reactive ones. All the agents we studied used a recurrent neural network, however, only the ones that used it to predict future outcomes (anticipative agents) were able to account for changes in the context of games, a behavior also observed in experiments with humans. This extended abstract aims to explain how we wish to investigate anticipation within the context of the collective-risk game and the relevance these results may have for the field of hybrid socio-technical systems.

",1,semantic_scholar
Dissecting Pruned Neural Networks,Jonathan Frankle (N/A); David Bau (N/A),2019,arXiv.org,,,1907.00262,"Pruning is a standard technique for removing unnecessary structure from a neural network to reduce its storage footprint, computational demands, or energy consumption. Pruning can reduce the parameter-counts of many state-of-the-art neural networks by an order of magnitude without compromising accuracy, meaning these networks contain a vast amount of unnecessary structure. In this paper, we study the relationship between pruning and interpretability. Namely, we consider the effect of removing unnecessary structure on the number of hidden units that learn disentangled representations of human-recognizable concepts as identified by network dissection. We aim to evaluate how the interpretability of pruned neural networks changes as they are compressed. We find that pruning has no detrimental effect on this measure of interpretability until so few parameters remain that accuracy beings to drop. Resnet-50 models trained on ImageNet maintain the same number of interpretable concepts and units until more than 90% of parameters have been pruned.",8,semantic_scholar
Interpretable Rotation-Equivariant Quaternion Neural Networks for 3D Point Cloud Processing,Wen Shen (N/A); Zhihua Wei (N/A); Qihan Ren (N/A); Binbin Zhang (N/A); Shikun Huang (N/A); Jiaqi Fan (N/A); Quanshi Zhang (N/A),2024,IEEE Transactions on Pattern Analysis and Machine Intelligence,10.1109/TPAMI.2023.3346383,38190688,,"This study proposes a set of generic rules to revise existing neural networks for 3D point cloud processing to rotation-equivariant quaternion neural networks (REQNNs), in order to make feature representations of neural networks to be rotation-equivariant and permutation-invariant. Rotation equivariance of features means that the feature computed on a rotated input point cloud is the same as applying the same rotation transformation to the feature computed on the original input point cloud. We find that the rotation-equivariance of features is naturally satisfied, if a neural network uses quaternion features. Interestingly, we prove that such a network revision also makes gradients of features in the REQNN to be rotation-equivariant w.r.t. inputs, and the training of the REQNN to be rotation-invariant w.r.t. inputs. Besides, permutation-invariance examines whether the intermediate-layer features are invariant, when we reorder input points. We also evaluate the stability of knowledge representations of REQNNs, and the robustness of REQNNs to adversarial rotation attacks. Experiments have shown that REQNNs outperform traditional neural networks in both terms of classification accuracy and robustness on rotated testing samples.",4,semantic_scholar
Neural Network with Forgetting: An ANN Algorithm for Customer,Q. Ye (N/A); Tao Lu (N/A); Yijun Li (N/A); Wenjun Sun (N/A),2005,Proceedings of the Annual Hawaii International Conference on System Sciences,10.1109/HICSS.2005.454,,,,2,semantic_scholar
Musical Instrument Recognition Using Their Distinctive Characteristics in Artificial Neural Networks,Babak Toghiani-Rizi (N/A); Marcus Windmark (N/A),2017,arXiv.org,,,1705.04971,"In this study an Artificial Neural Network was trained to classify musical instruments, using audio samples transformed to the frequency domain. Different features of the sound, in both time and frequency domain, were analyzed and compared in relation to how much information that could be derived from that limited data. The study concluded that in comparison with the base experiment, that had an accuracy of 93.5%, using the attack only resulted in 80.2% and the initial 100 Hz in 64.2%.",12,semantic_scholar
Codiscovering graphical structure and functional relationships within data: A Gaussian Process framework for connecting the dots,Théo Bourdais (N/A); Pau Batlle (N/A); Xianjin Yang (N/A); Ricardo Baptista (N/A); Nicolas Rouquette (N/A); H. Owhadi (N/A),2024,Proceedings of the National Academy of Sciences of the United States of America,10.1073/pnas.2403449121,39088394,,"Significance Many complex data analysis problems within and beyond the scientific domain involve discovering graphical structures and functional relationships within data. Nonlinear variance decomposition with Gaussian Processes simplifies and automates this process. Other methods, such as artificial neural networks, lack this variance decomposition feature. Information-theoretic and causal inference methods suffer from super-exponential complexity with respect to the number of variables. The proposed technique performs this task in polynomial complexity. This unlocks the potential for applications involving the identification of a network of hidden relationships between variables without a parameterized model at a remarkable scale, scope, and complexity.",6,semantic_scholar
Neural substrates of defensive reactivity in two subtypes of specific phobia.,U. Lueken (N/A); K. Hilbert (N/A); V. Stolyar (N/A); N. Maslowski (N/A); K. Beesdo-Baum (N/A); H. Wittchen (N/A),2014,Social Cognitive and Affective Neuroscience,10.1093/scan/nst159,24174207,,"Depending on threat proximity, different defensive behaviours are mediated by a descending neural network involving forebrain (distal threat) vs midbrain areas (proximal threat). Compared to healthy subjects, it can be assumed that phobics are characterized by shortened defensive distances on a behavioural and neural level. This study aimed at characterizing defensive reactivity in two subtypes of specific phobia [snake (SP) and dental phobics (DP)]. Using functional magnetic resonance imaging (fMRI), n = 39 subjects (13 healthy controls, HC; 13 SP; 13 DP) underwent an event-related fMRI task employing an anticipation (5-10 s) and immediate perception phase (phobic pictures and matched neutral stimuli; 1250 ms) to modulate defensive distance. Although no differential brain activity in any comparisons was observed in DP, areas associated with defensive behaviours (e.g. amygdala, hippocampus, midbrain) were activated in SP. Decreasing defensive distance in SP was characterized by a shift to midbrain activity. Present findings substantiate differences between phobia types in their physiological and neural organization that can be expanded to early stages of defensive behaviours. Findings may contribute to a better understanding of the dynamic organization of defensive reactivity in different types of phobic fear.",31,semantic_scholar
"Early Detection of Aphid Infestation and Insect-Plant Interaction Assessment in Wheat Using a Low-Cost Electronic Nose (E-Nose), Near-Infrared Spectroscopy and Machine Learning Modeling",S. Fuentes (N/A); E. Tongson (N/A); R. Unnithan (N/A); Claudia Gonzalez Viejo (N/A),2021,Italian National Conference on Sensors,10.3390/s21175948,34502839,,"Advances in early insect detection have been reported using digital technologies through camera systems, sensor networks, and remote sensing coupled with machine learning (ML) modeling. However, up to date, there is no cost-effective system to monitor insect presence accurately and insect-plant interactions. This paper presents results on the implementation of near-infrared spectroscopy (NIR) and a low-cost electronic nose (e-nose) coupled with machine learning. Several artificial neural network (ANN) models were developed based on classification to detect the level of infestation and regression to predict insect numbers for both e-nose and NIR inputs, and plant physiological response based on e-nose to predict photosynthesis rate (A), transpiration (E) and stomatal conductance (gs). Results showed high accuracy for classification models ranging within 96.5–99.3% for NIR and between 94.2–99.2% using e-nose data as inputs. For regression models, high correlation coefficients were obtained for physiological parameters (gs, E and A) using e-nose data from all samples as inputs (R = 0.86) and R = 0.94 considering only control plants (no insect presence). Finally, R = 0.97 for NIR and R = 0.99 for e-nose data as inputs were obtained to predict number of insects. Performances for all models developed showed no signs of overfitting. In this paper, a field-based system using unmanned aerial vehicles with the e-nose as payload was proposed and described for deployment of ML models to aid growers in pest management practices.",37,semantic_scholar
ABCNet: an attention-based method for particle tagging,V. Mikuni (N/A); F. Canelli (N/A),2020,The European Physical Journal Plus,10.1140/epjp/s13360-020-00497-3,32647596,2001.05311,"In high energy physics, graph-based implementations have the advantage of treating the input data sets in a similar way as they are collected by collider experiments. To expand on this concept, we propose a graph neural network enhanced by attention mechanisms called ABCNet. To exemplify the advantages and flexibility of treating collider data as a point cloud, two physically motivated problems are investigated: quark–gluon discrimination and pileup reduction. The former is an event-by-event classification, while the latter requires each reconstructed particle to receive a classification score. For both tasks, ABCNet shows an improved performance compared to other algorithms available.",83,semantic_scholar
A New CNN-Based Model for Financial Time Series: TAIEX and FTSE Stocks Forecasting,M. Kirisci (N/A); Ozge Cagcag Yolcu (N/A),2022,Neural Processing Letters,10.1007/s11063-022-10767-z,,,,45,semantic_scholar
Efficient 2D localization of a number of mutually arbitrary positioned stochastic EM sources in far-field using neural model,Z. Stanković (N/A); N. Dončov (N/A); B. Milovanovic (N/A); I. Milovanovic (N/A),2017,International Conference on Electromagnetics in Advanced Applications,10.1109/ICEAA.2017.8065537,,,,4,semantic_scholar
Neural network estimation of balance control during locomotion.,M. Hahn (N/A); A. Farley (N/A); Victor Lin (N/A); L. Chou (N/A),2005,Journal of Biomechanics,10.1016/J.JBIOMECH.2004.05.012,15713292,,,38,semantic_scholar
Sparse Adversarial Attack via Perturbation Factorization,Yanbo Fan (N/A); Baoyuan Wu (N/A); T. Li (N/A); Yong Zhang (N/A); Mingyang Li (N/A); Zhifeng Li (N/A); Yujiu Yang (N/A),2020,European Conference on Computer Vision,10.1007/978-3-030-58542-6_3,,,,84,semantic_scholar
Design and experimental evaluation of an intelligent PID controller using CMACs,K. Koiwai (N/A); K. Kawada (N/A); Toru Yamamoto (N/A),2009,"International Conference on Networking, Sensing and Control",10.1109/ICNSC.2009.4919370,,,,5,semantic_scholar
Novel algorithm for the real time multi-feature detection in laser beam welding,N. Leonardo (N/A); T. Ronald (N/A); Abt Felix (N/A); Heider Andreas (N/A); Blug Andreas (N/A); Hofler Heinrich (N/A),2012,International Symposium on Circuits and Systems,10.1109/ISCAS.2012.6271618,,,,9,semantic_scholar
On global exponential stability of cellular neural networks with Lipschitz-continuous activation function and variable delays,Dongming Zhou (N/A); Liming Zhang (N/A); Jinde Cao (N/A),2004,Applied Mathematics and Computation,10.1016/S0096-3003(03)00347-3,,,,28,semantic_scholar
Odorant-Evoked Nitric Oxide Signals in the Antennal Lobe of Manduca sexta,Chad Collmann (N/A); Mikael A. Carlsson (N/A); B. Hansson (N/A); A. Nighorn (N/A),2004,Journal of Neuroscience,10.1523/JNEUROSCI.0710-04.2004,15240798,,"The gaseous signaling molecule nitric oxide (NO) can affect the activities of neurons and neural networks in many different systems. The strong expression of NO synthase (NOS) in the primary synaptic neuropil (the antennal lobe in insects and the olfactory bulb in vertebrates) of the olfactory system of most organisms, and the unique spheroidal geometry of olfactory glomeruli in those neuropils, have led to suggestions that NO signaling is important for processing olfactory information. No direct evidence exists, however, that NO signals are produced in olfactory glomeruli. We investigated the production of NO in the antennal lobe of the moth, Manduca sexta, by using immunocytochemistry and real-time optical imaging with a NO-sensitive fluorescent marker, diaminofluorescein diacetate. We confirmed that NOS was expressed in the axons of olfactory receptor neurons projecting to all glomeruli. Soluble guanylyl cyclase, the best characterized target of NO, was found in a subset of postsynaptic antennal lobe neurons that included projection neurons, a small number of GABA-immunoreactive neurons, and a serotonin-immunoreactive neuron. We found that odorant stimulation evoked NO signals that were reproducible and spatially focused. Different odorants evoked spatially distinct patterns of NO production. Increased concentrations of pheromone and plant odorants caused increases in peak signal intensity. Increased concentrations of plant odorants also evoked a dramatic increase in signal area. The results of these experiments show clearly that odorant stimulation can evoke NO production in the olfactory system. The NO signals produced are likely to play an important role in processing olfactory information.",51,semantic_scholar
A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images using Choquet Integral and Differential Evolution Optimization,Amir Reza Takhsha (N/A); Maryam Rastgarpour (N/A); Mozhgan Naderi (N/A),2025,arXiv.org,10.48550/arXiv.2501.08241,,2501.08241,"The COVID-19 pandemic has profoundly impacted billions globally. It challenges public health and healthcare systems due to its rapid spread and severe respiratory effects. An effective strategy to mitigate the COVID-19 pandemic involves integrating testing to identify infected individuals. While RT-PCR is considered the gold standard for diagnosing COVID-19, it has some limitations such as the risk of false negatives. To address this problem, this paper introduces a novel Deep Learning Diagnosis System that integrates pre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble learning framework to achieve precise identification of COVID-19 cases from Chest X-ray (CXR) images. We combine feature vectors from the final hidden layers of pre-trained DCNNs using the Choquet integral to capture interactions between different DCNNs that a linear approach cannot. We employed Sugeno-$\lambda$ measure theory to derive fuzzy measures for subsets of networks to enable aggregation. We utilized Differential Evolution to estimate fuzzy densities. We developed a TensorFlow-based layer for Choquet operation to facilitate efficient aggregation, due to the intricacies involved in aggregating feature vectors. Experimental results on the COVIDx dataset show that our ensemble model achieved 98\% accuracy in three-class classification and 99.50\% in binary classification, outperforming its components-DenseNet-201 (97\% for three-class, 98.75\% for binary), Inception-v3 (96.25\% for three-class, 98.50\% for binary), and Xception (94.50\% for three-class, 98\% for binary)-and surpassing many previous methods.",0,semantic_scholar
Convolutional neural networks.,"Alexander Derry (Department of Biomedical Data Science, Stanford University, Stanford, CA, USA.); Martin Krzywinski (Canada's Michael Smith Genome Sciences Centre, Vancouver, British Columbia, Canada. martink@bcgsc.ca.); Naomi Altman (Department of Statistics, The Pennsylvania State University, State College, PA, USA.)",,Nature methods,10.1038/s41592-021-01232-1,37580560,,"{'text': '', 'structured': []}",0,pubmed
Convolutional Neural Networks for Radiologic Images: A Radiologist's Guide.,"Shelly Soffer (From the Department of Diagnostic Imaging, Sheba Medical Center, Emek HaEla St 1, Ramat Gan, Israel (S.S., M.M.A., E.K.); Faculty of Engineering, Department of Biomedical Engineering, Medical Image Processing Laboratory, Tel Aviv University, Tel Aviv, Israel (A.B., H.G.); and Sackler School of Medicine, Tel Aviv University, Tel Aviv, Israel (S.S., O.S.).); Avi Ben-Cohen (From the Department of Diagnostic Imaging, Sheba Medical Center, Emek HaEla St 1, Ramat Gan, Israel (S.S., M.M.A., E.K.); Faculty of Engineering, Department of Biomedical Engineering, Medical Image Processing Laboratory, Tel Aviv University, Tel Aviv, Israel (A.B., H.G.); and Sackler School of Medicine, Tel Aviv University, Tel Aviv, Israel (S.S., O.S.).); Orit Shimon (From the Department of Diagnostic Imaging, Sheba Medical Center, Emek HaEla St 1, Ramat Gan, Israel (S.S., M.M.A., E.K.); Faculty of Engineering, Department of Biomedical Engineering, Medical Image Processing Laboratory, Tel Aviv University, Tel Aviv, Israel (A.B., H.G.); and Sackler School of Medicine, Tel Aviv University, Tel Aviv, Israel (S.S., O.S.).); Michal Marianne Amitai (From the Department of Diagnostic Imaging, Sheba Medical Center, Emek HaEla St 1, Ramat Gan, Israel (S.S., M.M.A., E.K.); Faculty of Engineering, Department of Biomedical Engineering, Medical Image Processing Laboratory, Tel Aviv University, Tel Aviv, Israel (A.B., H.G.); and Sackler School of Medicine, Tel Aviv University, Tel Aviv, Israel (S.S., O.S.).); Hayit Greenspan (From the Department of Diagnostic Imaging, Sheba Medical Center, Emek HaEla St 1, Ramat Gan, Israel (S.S., M.M.A., E.K.); Faculty of Engineering, Department of Biomedical Engineering, Medical Image Processing Laboratory, Tel Aviv University, Tel Aviv, Israel (A.B., H.G.); and Sackler School of Medicine, Tel Aviv University, Tel Aviv, Israel (S.S., O.S.).); Eyal Klang (From the Department of Diagnostic Imaging, Sheba Medical Center, Emek HaEla St 1, Ramat Gan, Israel (S.S., M.M.A., E.K.); Faculty of Engineering, Department of Biomedical Engineering, Medical Image Processing Laboratory, Tel Aviv University, Tel Aviv, Israel (A.B., H.G.); and Sackler School of Medicine, Tel Aviv University, Tel Aviv, Israel (S.S., O.S.).)",,Radiology,10.1148/radiol.2018180547,30694159,,"{'text': 'Deep learning has rapidly advanced in various fields within the past few years and has recently gained particular attention in the radiology community. This article provides an introduction to deep learning technology and presents the stages that are entailed in the design process of deep learning radiology research. In addition, the article details the results of a survey of the application of deep learning-specifically, the application of convolutional neural networks-to radiologic imaging that was focused on the following five major system organs: chest, breast, brain, musculoskeletal system, and abdomen and pelvis. The survey of the studies is followed by a discussion about current challenges and future trends and their potential implications for radiology. This article may be used as a guide for radiologists planning research in the field of radiologic image analysis using convolutional neural networks.', 'structured': []}",0,pubmed
Neural networks for biomechanics.,"Madhura Mukhopadhyay (Nature Methods, . madhura.mukho@us.nature.com.)",,Nature methods,10.1038/s41592-024-02218-5,38472462,,"{'text': '', 'structured': []}",0,pubmed
Neural network models and deep learning.,"Nikolaus Kriegeskorte (Department of Psychology, Columbia University, New York, NY 10027, USA; Department of Neuroscience, Columbia University, New York, NY 10027, USA; Department of Electrical Engineering, Columbia University, New York, NY 10027, USA; Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA. Electronic address: n.kriegeskorte@columbia.edu.); Tal Golan (Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA. Electronic address: tal.golan@columbia.edu.)",,Current biology : CB,10.1016/j.cub.2019.02.034,30939301,,"{'text': 'Originally inspired by neurobiology, deep neural network models have become a powerful tool of machine learning and artificial intelligence. They can approximate functions and dynamics by learning from examples. Here we give a brief introduction to neural network models and deep learning for biologists. We introduce feedforward and recurrent networks and explain the expressive power of this modeling framework and the backpropagation algorithm for setting the parameters. Finally, we consider how deep neural network models might help us understand brain computation.', 'structured': []}",0,pubmed
Artificial neural networks and deep learning.,"Melvin Geubbelmans (Hasselt, Belgium, and Bialystok, Poland.); Axel-Jan Rousseau (Hasselt, Belgium, and Bialystok, Poland.); Tomasz Burzykowski (Hasselt, Belgium, and Bialystok, Poland.); Dirk Valkenborg (Hasselt, Belgium, and Bialystok, Poland.)",,"American journal of orthodontics and dentofacial orthopedics : official publication of the American Association of Orthodontists, its constituent societies, and the American Board of Orthodontics",10.1016/j.ajodo.2023.11.003,38302219,,"{'text': '', 'structured': []}",0,pubmed
Self-Replication in Neural Networks.,Thomas Gabor (LMU Munich. thomas.gabor@ifi.lmu.de.); Steffen Illium (LMU Munich.); Maximilian Zorn (LMU Munich.); Cristian Lenta (LMU Munich.); Andy Mattausch (LMU Munich.); Lenz Belzner (Technische Hochschule Ingolstadt.); Claudia Linnhoff-Popien (LMU Munich.),,Artificial life,10.1162/artl_a_00359,35727999,,"{'text': ""A key element of biological structures is self-replication. Neural networks are the prime structure used for the emergent construction of complex behavior in computers. We analyze how various network types lend themselves to self-replication. Backpropagation turns out to be the natural way to navigate the space of network weights and allows non-trivial self-replicators to arise naturally. We perform an in-depth analysis to show the self-replicators' robustness to noise. We then introduce artificial chemistry environments consisting of several neural networks and examine their emergent behavior. In extension to this work's previous version (Gabor et al., 2019), we provide an extensive analysis of the occurrence of fixpoint weight configurations within the weight space and an approximation of their respective attractor basins."", 'structured': []}",0,pubmed
Approximation capabilities of measure-preserving neural networks.,"Aiqing Zhu (LSEC, ICMSEC, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China; School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing 100049, China.); Pengzhan Jin (LSEC, ICMSEC, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China; School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing 100049, China.); Yifa Tang (LSEC, ICMSEC, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China; School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing 100049, China. Electronic address: tyf@lsec.cc.ac.cn.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2021.12.007,34995951,,"{'text': 'Measure-preserving neural networks are well-developed invertible models, however, their approximation capabilities remain unexplored. This paper rigorously analyzes the approximation capabilities of existing measure-preserving neural networks including NICE and RevNets. It is shown that for compact U⊂R', 'structured': []}",0,pubmed
Deep Learning for Epidemiologists: An Introduction to Neural Networks.,Stylianos Serghiou (N/A); Kathryn Rough (N/A),,American journal of epidemiology,10.1093/aje/kwad107,37139570,,"{'text': ""Deep learning methods are increasingly being applied to problems in medicine and health care. However, few epidemiologists have received formal training in these methods. To bridge this gap, this article introduces the fundamentals of deep learning from an epidemiologic perspective. Specifically, this article reviews core concepts in machine learning (e.g., overfitting, regularization, and hyperparameters); explains several fundamental deep learning architectures (convolutional neural networks, recurrent neural networks); and summarizes training, evaluation, and deployment of models. Conceptual understanding of supervised learning algorithms is the focus of the article; instructions on the training of deep learning models and applications of deep learning to causal learning are out of this article's scope. We aim to provide an accessible first step towards enabling the reader to read and assess research on the medical applications of deep learning and to familiarize readers with deep learning terminology and concepts to facilitate communication with computer scientists and machine learning engineers."", 'structured': []}",0,pubmed
Separable integral neural networks.,"Jinhua Lin (Department of Computer Application Technology, Changchun University of Technology, PR China. Electronic address: linjinhua@ccut.edu.cn.); Xin Li (Department of Computer Application Technology, Changchun University of Technology, PR China.); Lin Ma (First Automobile Works Group Corp, PR China.); Bowen Ren (Department of Computer Application Technology, Changchun University of Technology, PR China.); Xiangdong Hao (Department of Computer Application Technology, Changchun University of Technology, PR China.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2024.106838,39615156,,"{'text': 'Integral neural networks adopt continuous integral operators instead of conventional discrete convolutional operations to perform deep learning tasks. As this integral operator is the continuous representation of the regular convolutional operation, it is not suitable for representing the separable convolutional operations widely deployed on mobile devices. To address this issue, a separable integral layer composed of a depth-wise integral operator and a point-wise integral operator is proposed in this paper to represent discrete depth-wise and point-wise convolutional operations in continuous manner. According to the fabric units of five classical convolutional neural networks(NIN, VGG11, GoogleNet, ResNet18, ResNet50), we design five kinds of separable integral blocks(SIBs) to encapsulate separable integral layers in different manner. Using the proposed SIBs as basic blocks, a family of lightweight separable integral neural networks(SINNs) are constructed and deployed on resource-constrained mobile devices. SINNs have the characteristics of integral neural networks, i.e., performing structural pruning without fine-tuning, and also inherit the advantages of separable convolutional operations, i.e., reducing the computational cost while keeping a competitive performance. The experimental results show that SINNs achieve the similar performance with the state-of-the-art integral neural networks(INNs), while reducing the computational cost to up to 1/1.79 times that of INN(1.74× fewer parameters than INN using ResNet101 backbone framework) on ImageNet dataset. The code will be released at https://github.com/ljh3832-ccut/SINN.', 'structured': []}",0,pubmed
Graph neural networks in histopathology: Emerging trends and future directions.,"Siemen Brussee (Leiden University Medical Center, Albinusdreef 2, 2333 ZA, Leiden, The Netherlands. Electronic address: s.brussee@lumc.nl.); Giorgio Buzzanca (Leiden University Medical Center, Albinusdreef 2, 2333 ZA, Leiden, The Netherlands.); Anne M R Schrader (Leiden University Medical Center, Albinusdreef 2, 2333 ZA, Leiden, The Netherlands.); Jesper Kers (Leiden University Medical Center, Albinusdreef 2, 2333 ZA, Leiden, The Netherlands; Amsterdam University Medical Center, Meibergdreef 9, 1105 AZ, Amsterdam, The Netherlands.)",,Medical image analysis,10.1016/j.media.2024.103444,39793218,,"{'text': 'Histopathological analysis of whole slide images (WSIs) has seen a surge in the utilization of deep learning methods, particularly Convolutional Neural Networks (CNNs). However, CNNs often fail to capture the intricate spatial dependencies inherent in WSIs. Graph Neural Networks (GNNs) present a promising alternative, adept at directly modeling pairwise interactions and effectively discerning the topological tissue and cellular structures within WSIs. Recognizing the pressing need for deep learning techniques that harness the topological structure of WSIs, the application of GNNs in histopathology has experienced rapid growth. In this comprehensive review, we survey GNNs in histopathology, discuss their applications, and explore emerging trends that pave the way for future advancements in the field. We begin by elucidating the fundamentals of GNNs and their potential applications in histopathology. Leveraging quantitative literature analysis, we explore four emerging trends: Hierarchical GNNs, Adaptive Graph Structure Learning, Multimodal GNNs, and Higher-order GNNs. Through an in-depth exploration of these trends, we offer insights into the evolving landscape of GNNs in histopathological analysis. Based on our findings, we propose future directions to propel the field forward. Our analysis serves to guide researchers and practitioners towards innovative approaches and methodologies, fostering advancements in histopathological analysis through the lens of graph neural networks.', 'structured': []}",0,pubmed
Advancing drug discovery with deep attention neural networks.,"Antonio Lavecchia (Drug Discovery Laboratory, Department of Pharmacy, University of Napoli Federico II, I-80131 Naples, Italy. Electronic address: antonio.lavecchia@unina.it.)",,Drug discovery today,10.1016/j.drudis.2024.104067,38925473,,"{'text': 'In the dynamic field of drug discovery, deep attention neural networks are revolutionizing our approach to complex data. This review explores the attention mechanism and its extended architectures, including graph attention networks (GATs), transformers, bidirectional encoder representations from transformers (BERT), generative pre-trained transformers (GPTs) and bidirectional and auto-regressive transformers (BART). Delving into their core principles and multifaceted applications, we uncover their pivotal roles in catalyzing de novo drug design, predicting intricate molecular properties and deciphering elusive drug-target interactions. Despite challenges, these attention-based architectures hold unparalleled promise to drive transformative breakthroughs and accelerate progress in pharmaceutical research.', 'structured': []}",0,pubmed
Transforming clinical cardiology through neural networks and deep learning: A guide for clinicians.,"Henry Sutanto (Department of Internal Medicine, Faculty of Medicine, Universitas Airlangga, Surabaya, Indonesia. Electronic address: henry.sutanto-2022@fk.unair.ac.id.)",,Current problems in cardiology,10.1016/j.cpcardiol.2024.102454,38342351,,"{'text': ""The rapid evolution of neural networks and deep learning has revolutionized various fields, with clinical cardiology being no exception. As traditional methods in cardiology encounter limitations, the integration of advanced computational techniques offers unprecedented opportunities in diagnostics and patient care. This review explores the transformative role of neural networks and deep learning in clinical cardiology, particularly focusing on their applications in electrocardiogram (ECG) analysis, imaging technologies, and cardiac prediction models. Among others, Deep Neural Networks (DNNs) have significantly surpassed traditional approaches in accuracy and efficiency in automatic ECG diagnosis. Convolutional Neural Networks (CNNs) are successfully applied in PET/CT and PET/MR imaging, enhancing diagnostic capabilities. Furthermore, deep learning algorithms have shown potential in improving cardiac prediction models, although challenges in interpretability and clinical integration remain. The review also addresses the 'black box' nature of neural networks and the ethical considerations surrounding their use in clinical settings. Overall, this review underscores the significant impact of neural networks and deep learning in cardiology, providing insights into current applications and future directions in the field."", 'structured': []}",0,pubmed
Image Sensing and Processing with Convolutional Neural Networks.,"Sonya Coleman (School of Computing, Engineering and Intelligent Systems, Ulster University, Londonderry BT48 7JL, UK.); Dermot Kerr (School of Computing, Engineering and Intelligent Systems, Ulster University, Londonderry BT48 7JL, UK.); Yunzhou Zhang (College of Information Science and Engineering, Northeastern University, Shenyang 110819, China.)",,"Sensors (Basel, Switzerland)",10.3390/s21062132,35632021,,"{'text': 'Convolutional neural networks are a class of deep neural networks that leverage spatial information, and they are therefore well suited to classifying images for a range of applications [...].', 'structured': []}",0,pubmed
Neural networks.,M F Jefferson (N/A); N Pendleton (N/A); S Lucas (N/A); M A Horan (N/A),,"Lancet (London, England)",,8551860,,"{'text': '', 'structured': []}",0,pubmed
An overview of neural networks for drug discovery and the inputs used.,"Yinqiu Xu (a Department of Medicinal Chemistry, School of Pharmacy , China Pharmaceutical University , Nanjing , China.); Hequan Yao (a Department of Medicinal Chemistry, School of Pharmacy , China Pharmaceutical University , Nanjing , China.); Kejiang Lin (a Department of Medicinal Chemistry, School of Pharmacy , China Pharmaceutical University , Nanjing , China.)",,Expert opinion on drug discovery,10.1080/17460441.2018.1547278,30449189,,"{'text': '', 'structured': []}",0,pubmed
Rethinking arithmetic for deep neural networks.,"G A Constantinides (EEE Department, Imperial College London, London, UK.)",,"Philosophical transactions. Series A, Mathematical, physical, and engineering sciences",10.1613/jair.105,31955685,,"{'text': 'We consider efficiency in the implementation of deep neural networks. Hardware accelerators are gaining interest as machine learning becomes one of the drivers of high-performance computing. In these accelerators, the directed graph describing a neural network can be implemented as a directed graph describing a Boolean circuit. We make this observation precise, leading naturally to an understanding of practical neural networks as discrete functions, and show that the so-called ', 'structured': []}",0,pubmed
Convergence of deep convolutional neural networks.,"Yuesheng Xu (Department of Mathematics & Statistics, Old Dominion University, Norfolk, VA 23529, USA. Electronic address: y1xu@odu.edu.); Haizhang Zhang (School of Mathematics (Zhuhai), Sun Yat-sen University, Zhuhai, PR China. Electronic address: zhhaizh2@sysu.edu.cn.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2022.06.031,35839599,,"{'text': 'Convergence of deep neural networks as the depth of the networks tends to infinity is fundamental in building the mathematical foundation for deep learning. In a previous study, we investigated this question for deep networks with the Rectified Linear Unit (ReLU) activation function and with a fixed width. This does not cover the important convolutional neural networks where the widths are increased from layer to layer. For this reason, we first study convergence of general ReLU networks with increased widths and then apply the results obtained to deep convolutional neural networks. It turns out the convergence reduces to convergence of infinite products of matrices with increased sizes, which has not been considered in the literature. We establish sufficient conditions for convergence of such infinite products of matrices. Based on the conditions, we present sufficient conditions for pointwise convergence of general deep ReLU networks with increasing widths, and as well as pointwise convergence of deep ReLU convolutional neural networks.', 'structured': []}",0,pubmed
Neural networks are useful tools for drug design.,"G Schneider (F. Hoffmann-La Roche Ltd., Pharmaceuticals Research, Basel, Switzerland. gisbert.schneider@roche.com)",,Neural networks : the official journal of the International Neural Network Society,10.1016/s0893-6080(99)00094-5,10935454,,"{'text': 'A profound understanding of molecular recognition processes and the underlying molecular interaction patterns is a prerequisite for future progress and success in rational drug design. Neural networks could play an important role in guiding the Drug Discovery process through the extraction of relevant molecular features.', 'structured': []}",0,pubmed
Optical computing powers graph neural networks.,Kaida Tang (N/A); Jianwei Chen (N/A); Huaqing Jiang (N/A); Jun Chen (N/A); Shangzhong Jin (N/A); Ran Hao (N/A),,Applied optics,10.1364/AO.475991,36607108,,"{'text': 'Graph-based neural networks have promising perspectives but are limited by electronic bottlenecks. Our work explores the advantages of optical neural networks in the graph domain. We propose an optical graph neural network (OGNN) based on inverse-designed optical processing units (OPUs) to classify graphs with optics. The OPUs, combined with two types of optical components, can perform multiply-accumulate, matrix-vector multiplication, and matrix-matrix multiplication operations. The proposed OGNN can classify typical non-Euclidean MiniGCDataset graphs and successfully predict 1000 test graphs with 100% accuracy. The OPU-formed optical-electrical graph attention network is also scalable to handle more complex graph data, such as the Cora dataset, with 89.0% accuracy.', 'structured': []}",0,pubmed
The capacity of feedforward neural networks.,"Pierre Baldi (Department of Computer Science, University of California, Irvine, United States. Electronic address: pfbaldi@uci.edu.); Roman Vershynin (Department of Mathematics, University of California, Irvine, United States. Electronic address: rvershyn@uci.edu.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2019.04.009,31125915,,"{'text': 'A long standing open problem in the theory of neural networks is the development of quantitative methods to estimate and compare the capabilities of different architectures. Here we define the capacity of an architecture by the binary logarithm of the number of functions it can compute, as the synaptic weights are varied. The capacity provides an upperbound on the number of bits that can be extracted from the training data and stored in the architecture during learning. We study the capacity of layered, fully-connected, architectures of linear threshold neurons with L layers of size n', 'structured': []}",0,pubmed
Spiking neural networks take control.,"Travis DeWolf (Applied Brain Research Inc., Toronto, Ontario, Canada. Email: travis.dewolf@appliedbrainresearch.com.)",,Science robotics,10.1126/scirobotics.abk3268,34516751,,"{'text': 'Brain-inspired neural network architecture overcomes unsolved classical control theory problem for telerobotics.', 'structured': []}",0,pubmed
Augmenting Recurrent Neural Networks Resilience by Dropout.,Davide Bacciu (N/A); Francesco Crecchi (N/A),,IEEE transactions on neural networks and learning systems,10.1109/TNNLS.2019.2899744,30892245,,"{'text': 'This brief discusses the simple idea that dropout regularization can be used to efficiently induce resiliency to missing inputs at prediction time in a generic neural network. We show how the approach can be effective on tasks where imputation strategies often fail, namely, involving recurrent neural networks and scenarios where whole sequences of input observations are missing. The experimental analysis provides an assessment of the accuracy-resiliency tradeoff in multiple recurrent models, including reservoir computing methods, and comprising real-world ambient intelligence and biomedical time series.', 'structured': []}",0,pubmed
Deep neural networks and humans both benefit from compositional language structure.,"Lukas Galke (Department of Mathematics and Computer Science, University of Southern Denmark, Odense, Denmark. galke@imada.sdu.dk.); Yoav Ram (School of Zoology, Faculty of Life Sciences, Tel Aviv University, Tel Aviv, Israel.); Limor Raviv (LEADS group, Max Planck Institute for Psycholinguistics, Nijmegen, Netherlands.)",,Nature communications,10.1038/s41467-024-55158-1,39738033,,"{'text': 'Deep neural networks drive the success of natural language processing. A fundamental property of language is its compositional structure, allowing humans to systematically produce forms for new meanings. For humans, languages with more compositional and transparent structures are typically easier to learn than those with opaque and irregular structures. However, this learnability advantage has not yet been shown for deep neural networks, limiting their use as models for human language learning. Here, we directly test how neural networks compare to humans in learning and generalizing different languages that vary in their degree of compositional structure. We evaluate the memorization and generalization capabilities of a large language model and recurrent neural networks, and show that both deep neural networks exhibit a learnability advantage for more structured linguistic input: neural networks exposed to more compositional languages show more systematic generalization, greater agreement between different agents, and greater similarity to human learners.', 'structured': []}",0,pubmed
Editorial introduction to the Neural Networks special issue on Deep Learning of Representations.,Yoshua Bengio (N/A); Honglak Lee (N/A),,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2014.12.006,25595998,,"{'text': '', 'structured': []}",0,pubmed
Quaternion Spiking and Quaternion Quantum Neural Networks: Theory and Applications.,"Eduardo Bayro-Corrochano (Department of Electrical Engineering and Computer Science, CINVESTAV Guadalajara, México.); Samuel Solis-Gamboa (Department of Electrical Engineering and Computer Science, CINVESTAV Guadalajara, México.); Guillermo Altamirano-Escobedo (Department of Electrical Engineering and Computer Science, CINVESTAV Guadalajara, México.); Luis Lechuga-Gutierres (Department of Electrical Engineering and Computer Science, CINVESTAV Guadalajara, México.); Jorge Lisarraga-Rodriguez (Department of Electrical Engineering and Computer Science, CINVESTAV Guadalajara, México.)",,International journal of neural systems,10.1142/S0129065720500598,32938264,,"{'text': 'Biological evidence shows that there are neural networks specialized for recognition of signals and patterns acting as associative memories. The spiking neural networks are another kind which receive input from a broad range of other brain areas to produce output that selects particular cognitive or motor actions to perform. An important contribution of this work is to consider the geometric processing in the modeling of feed-forward neural networks. Since quaternions are well suited to represent 3D rotations, it is then well justified to extend real-valued neural networks to quaternion-valued neural networks for task of perception and control of robot manipulators. This work presents the quaternion spiking neural networks which are able to control robots, where the examples confirm that these artificial neurons have the capacity to adapt on-line the robot to reach the desired position. Also, we present the quaternionic quantum neural networks for pattern recognition using just one quaternion neuron. In the experimental analysis, we show the excellent performance of both quaternion neural networks.', 'structured': []}",0,pubmed
O(2) -Valued Hopfield Neural Networks.,Masaki Kobayashi (N/A),,IEEE transactions on neural networks and learning systems,10.1109/TNNLS.2019.2897994,30843853,,"{'text': 'In complex-valued Hopfield neural networks (CHNNs), the neuron states are complex numbers whose amplitudes are: 1) they can also be described in special orthogonal matrices of order and 2) here, we propose a new Hopfield model, the O(2) -valued Hopfield neural network [ O(2) -HNN], whose neuron states are extended to orthogonal matrices. Its neuron states are embedded in 4-D space, while those of CHNNs are embedded in 2-D space. Computer simulations were conducted to compare the noise tolerance (NT) and storage capacity (SC) of CHNNs, O(2) -HNNs, and rotor Hopfield neural networks. In terms of SC, O(2) -HNNs outperformed the others, while in NT, they outdid CHNNs.', 'structured': []}",0,pubmed
Neural networks: what are they?,M R Guerriere (N/A); A S Detsky (N/A),,Annals of internal medicine,10.7326/0003-4819-115-11-906,1952481,,"{'text': '', 'structured': []}",0,pubmed
Riemannian Curvature of Deep Neural Networks.,Piyush Kaul (N/A); Brejesh Lall (N/A),,IEEE transactions on neural networks and learning systems,10.1109/TNNLS.2019.2919705,31251199,,"{'text': 'We analyze deep neural networks using the theory of Riemannian geometry and curvature. The objective is to gain insight into how Riemannian geometry can characterize and predict the trained behavior of neural networks. We define a method for calculating Riemann and Ricci curvature tensors, and Ricci scalar curvature values for a trained neural net, in such a way that the output classifier softmax values are related to the input transformations, through the curvature equations. We also measure these curvature tensors experimentally for different networks which are pretrained with stochastic gradient descent and offer a way of visualizing and understanding the measurements to gain insight into the effect curvature has on behavior the neural networks locally, and possibly predict their behavior for different transformations of the test data. We also analyze the effect of variation in depth of the neural networks as well as how it behaves for different choices of data set.', 'structured': []}",0,pubmed
Multiscale Conditional Regularization for Convolutional Neural Networks.,Yao Lu (N/A); Guangming Lu (N/A); Jinxing Li (N/A); Yuanrong Xu (N/A); Zheng Zhang (N/A); David Zhang (N/A),,IEEE transactions on cybernetics,10.1109/TCYB.2020.2979968,32248140,,"{'text': 'With the increased model size of convolutional neural networks (CNNs), overfitting has become the main bottleneck to further improve the performance of networks. Currently, the weighting regularization methods have been proposed to address the overfitting problem and they perform satisfactorily. Since these regularization methods cannot be used in all the networks and they are usually not flexible enough in different phases of the training and test processes, this article proposes a multiscale conditional (MSC) regularization method. MSC divides the intermediate features into different scales and then generates new data for each scale features, respectively. In addition, the new data are generated by employing the information from two conditions: 1) each sample feature and 2) each layer pattern. Finally, a self-identity structure is proposed to supplement the features with the generated data. Therefore, MSC can adaptively and efficiently generate much finer and individualized data to make the entire regularization more flexible. Furthermore, MSC is more general and can be applied to all kinds of networks through the proposed self-identity structure. The experimental results on all the benchmark datasets showed that the proposed MSC regularization method achieves the best performances in all the networks.', 'structured': []}",0,pubmed
Towards Deeper Neural Networks for Neonatal Seizure Detection.,Aengus Daly (N/A); Alison O'Shea (N/A); Gordon Lightbody (N/A); Andriy Temko (N/A),,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,10.1109/EMBC46164.2021.9629485,34891440,,"{'text': 'Machine learning and more recently deep learning have become valuable tools in clinical decision making for neonatal seizure detection. This work proposes a deep neural network architecture which is capable of extracting information from long segments of EEG. Residual connections as well as data augmentation and a more robust optimizer are efficiently exploited to train a deeper architecture with an increased receptive field and longer EEG input. The proposed system is tested on a large clinical dataset of 4,570 hours of duration and benchmarked on a publicly available Helsinki dataset of 112 hours duration. The performance has improved from an AUC of 95.41% to an AUC of 97.73% when compared to a deep learning baseline.', 'structured': []}",0,pubmed
An adaptive device for AI neural networks.,"Rohit Abraham John (Department of Chemistry and Applied Biosciences, Institute of Inorganic Chemistry, ETH Zürich, CH-8093 Zürich, Switzerland.)",,"Science (New York, N.Y.)",10.1126/science.abn6196,35113694,,"{'text': '[Figure: see text].', 'structured': []}",0,pubmed
Could deep learning in neural networks improve the QSAR models?,"G Gini (DEIB, Politecnico di Milano, Milan, Italy.); F Zanoli (DEIB, Politecnico di Milano, Milan, Italy.); A Gamba (Istituto di Ricerche Farmacologiche Mario Negri IRCCS, Laboratory of Environmental Chemistry and Toxicology, Milan, Italy.); G Raitano (Istituto di Ricerche Farmacologiche Mario Negri IRCCS, Laboratory of Environmental Chemistry and Toxicology, Milan, Italy.); E Benfenati (Istituto di Ricerche Farmacologiche Mario Negri IRCCS, Laboratory of Environmental Chemistry and Toxicology, Milan, Italy.)",,SAR and QSAR in environmental research,10.1080/1062936X.2019.1650827,31460798,,"{'text': 'Assessing chemical toxicity is a multidisciplinary process, traditionally involving in vivo, in vitro and in silico tests. Currently, toxicological goal is to reduce new tests on chemicals, exploiting all information yet available. Recent advancements in machine learning and deep neural networks allow computers to automatically mine patterns and learn from data. This technology, applied to (Q)SAR model development, leads to discover by learning the structural-chemical-biological relationships and the emergent properties. Starting from Toxception, a deep neural network predicting activity from the chemical graph image, we designed SmilesNet, a recurrent neural network taking SMILES as the only input. We then integrated the two networks into C-Tox network to make the final classification. Results of our networks, trained on a ~20K molecule dataset with Ames test experimental values, match or even outperform the current state of the art. We also extract knowledge from the networks and compare it with the available mutagenic structural alerts. The advantage over traditional QSAR modelling is that our models automatically extract the features without using descriptors. Nevertheless, the model is successful if large numbers of examples are provided and computation is more complex than in classical methods.', 'structured': []}",0,pubmed
The Use of Deep Learning and Neural Networks in Imaging: Welcome to the New Mathematical Milieu of Medicine.,"Irvin M Modlin (Yale University School of Medicine, New Haven, Connecticut, USA, imodlin@irvinmodlin.com.); Mark Kidd (Wren Laboratories, Branford, Connecticut, USA.); Ignat A Drozdov (Wren Laboratories, Branford, Connecticut, USA.); Lisa Bodei (Memorial Sloan Kettering Cancer Center, New York, New York, USA.)",,Neuroendocrinology,10.1159/000504605,31694034,,"{'text': '', 'structured': []}",0,pubmed
Graph Neural Networks for ADHD Identification in Children.,"Runxuan Yu (Vanderbilt University, Nashville, TN, USA.); Xinmeng Zhang (Vanderbilt University, Nashville, TN, USA.); You Chen (Vanderbilt University, Nashville, TN, USA.)",,Studies in health technology and informatics,10.3233/SHTI251208,40776225,,"{'text': ""Attention-Deficit/Hyperactivity Disorder (ADHD) is a common neurodevelopmental disorder requiring early, accurate diagnosis for timely intervention. We evaluated GraphSAGE, a Graph Neural Network, against XGBoost using data from 42,041 children in the 2022 National Survey of Children's Health. GraphSAGE outperformed XGBoost (F1 score: 0.8331 vs. 0.8241)."", 'structured': []}",0,pubmed
Efficient Approximation of High-Dimensional Functions With Neural Networks.,Patrick Cheridito (N/A); Arnulf Jentzen (N/A); Florian Rossmannek (N/A),,IEEE transactions on neural networks and learning systems,10.1109/TNNLS.2021.3049719,33513112,,"{'text': 'In this article, we develop a framework for showing that neural networks can overcome the curse of dimensionality in different high-dimensional approximation problems. Our approach is based on the notion of a catalog network, which is a generalization of a standard neural network in which the nonlinear activation functions can vary from layer to layer as long as they are chosen from a predefined catalog of functions. As such, catalog networks constitute a rich family of continuous functions. We show that under appropriate conditions on the catalog, catalog networks can efficiently be approximated with rectified linear unit-type networks and provide precise estimates on the number of parameters needed for a given approximation accuracy. As special cases of the general results, we obtain different classes of functions that can be approximated with recitifed linear unit networks without the curse of dimensionality.', 'structured': []}",0,pubmed
Neural networks.,D F Signorini (N/A); J M Slattery (N/A),,"Lancet (London, England)",,7491032,,"{'text': '', 'structured': []}",0,pubmed
Neural networks.,L Tarassenko (N/A),,"Lancet (London, England)",,8551861,,"{'text': '', 'structured': []}",0,pubmed
Moving sampling physics-informed neural networks induced by moving mesh PDE.,"Yu Yang (School of Mathematics, Sichuan University, 610065, Chengdu, China. Electronic address: yangyu1@stu.scu.edu.cn.); Qihong Yang (School of Mathematics, Sichuan University, 610065, Chengdu, China. Electronic address: yangqh@stu.scu.edu.cn.); Yangtao Deng (School of Mathematics, Sichuan University, 610065, Chengdu, China. Electronic address: ytdeng1998@foxmail.com.); Qiaolin He (School of Mathematics, Sichuan University, 610065, Chengdu, China. Electronic address: qlhejenny@scu.edu.cn.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2024.106706,39270348,,"{'text': 'In this work, we propose an end-to-end adaptive sampling framework based on deep neural networks and the moving mesh method (MMPDE-Net), which can adaptively generate new sampling points by solving the moving mesh PDE. This model focuses on improving the quality of sampling points generation. Moreover, we develop an iterative algorithm based on MMPDE-Net, which makes sampling points distribute more precisely and controllably. Since MMPDE-Net is independent of the deep learning solver, we combine it with physics-informed neural networks (PINN) to propose moving sampling PINN (MS-PINN) and show the error estimate of our method under some assumptions. Finally, we demonstrate the performance improvement of MS-PINN compared to PINN through numerical experiments of four typical examples, which numerically verify the effectiveness of our method.', 'structured': []}",0,pubmed
Quantized Convolutional Neural Networks Robustness under Perturbation.,"Jack Langille (Department of Engineering Mathematics and Internetworking, Dalhousie University, Halifax, Nova Scotia, Canada.); Issam Hammad (Department of Engineering Mathematics and Internetworking, Dalhousie University, Halifax, Nova Scotia, Canada.); Guy Kember (Department of Engineering Mathematics and Internetworking, Dalhousie University, Halifax, Nova Scotia, Canada.)",,F1000Research,10.1109/TNET.2021.3137084,40308295,,"{'text': ""Contemporary machine learning models are increasingly becoming restricted by size and subsequent operations per forward pass, demanding increasing compute requirements. Quantization has emerged as a convenient approach to addressing this, in which weights and activations are mapped from their conventionally used floating-point 32-bit numeric representations to lower precision integers. This process introduces significant reductions in inference time and simplifies the hardware requirements. It is a well-studied result that the performance of such reduced precision models is congruent with their floating-point counterparts. However, there is a lack of literature that addresses the performance of quantized models in a perturbed input space, as is common when stress testing regular full-precision models, particularly for real-world deployments. We focus on addressing this gap in the context of 8-bit quantized convolutional neural networks (CNNs). We study three state-of-the-art CNNs: ResNet-18, VGG-16, and SqueezeNet1_1, and subject their floating point and fixed point forms to various noise regimes with varying intensities. We characterize performance in terms of traditional metrics, including top-1 and top-5 accuracy, as well as the F1 score. We also introduce a new metric, the Kullback-Liebler divergence of the two output distributions for a given floating-point/fixed-point model pair, as a means to examine how the model's output distribution has changed as a result of quantization, which, we contend, can be interpreted as a proxy for model similarity in decision making. We find that across all three models and under each perturbation scheme, the relative error between the quantized and full-precision model was consistently low. We also find that Kullback-Liebler divergence was on the same order of magnitude as the unperturbed tests across all perturbation regimes except Brownian noise, where significant divergences were observed for VGG-16 and SqueezeNet1_1."", 'structured': []}",0,pubmed
GradFreeBits: Gradient-Free Bit Allocation for Mixed-Precision Neural Networks.,"Benjamin Jacob Bodner (Department of Computer Science, Ben-Gurion University, Beer Sheva 8410501, Israel.); Gil Ben-Shalom (Department of Computer Science, Ben-Gurion University, Beer Sheva 8410501, Israel.); Eran Treister (Department of Computer Science, Ben-Gurion University, Beer Sheva 8410501, Israel.)",,"Sensors (Basel, Switzerland)",10.1109/CVPR.2018.00474,36560141,,"{'text': 'Quantized neural networks (QNNs) are among the main approaches for deploying deep neural networks on low-resource edge devices. Training QNNs using different levels of precision throughout the network (mixed-precision quantization) typically achieves superior trade-offs between performance and computational load. However, optimizing the different precision levels of QNNs can be complicated, as the values of the bit allocations are discrete and difficult to differentiate for. Moreover, adequately accounting for the dependencies between the bit allocation of different layers is not straightforward. To meet these challenges, in this work, we propose GradFreeBits: a novel joint optimization scheme for training mixed-precision QNNs, which alternates between gradient-based optimization for the weights and gradient-free optimization for the bit allocation. Our method achieves a better or on par performance with the current state-of-the-art low-precision classification networks on CIFAR10/100 and ImageNet, semantic segmentation networks on Cityscapes, and several graph neural networks benchmarks. Furthermore, our approach can be extended to a variety of other applications involving neural networks used in conjunction with parameters that are difficult to optimize for.', 'structured': []}",0,pubmed
Approximation rates for neural networks with encodable weights in smoothness spaces.,"Ingo Gühring (Institute of Mathematics, Technical University of Berlin, Straße des 17. Juni 136, 10623 Berlin, Germany. Electronic address: guehring@math.tu-berlin.de.); Mones Raslan (Institute of Mathematics, Technical University of Berlin, Straße des 17. Juni 136, 10623 Berlin, Germany. Electronic address: raslan@math.tu-berlin.de.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2020.11.010,33310376,,"{'text': 'We examine the necessary and sufficient complexity of neural networks to approximate functions from different smoothness spaces under the restriction of encodable network weights. Based on an entropy argument, we start by proving lower bounds for the number of nonzero encodable weights for neural network approximation in Besov spaces, Sobolev spaces and more. These results are valid for all sufficiently smooth activation functions. Afterwards, we provide a unifying framework for the construction of approximate partitions of unity by neural networks with fairly general activation functions. This allows us to approximate localized Taylor polynomials by neural networks and make use of the Bramble-Hilbert Lemma. Based on our framework, we derive almost optimal upper bounds in higher-order Sobolev norms. This work advances the theory of approximating solutions of partial differential equations by neural networks.', 'structured': []}",0,pubmed
Predicting Age with Deep Neural Networks from Polysomnograms.,Andreas Brink-Kjaer (N/A); Emmanuel Mignot (N/A); Helge B D Sorensen (N/A); Poul Jennum (N/A),,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,10.1109/EMBC44109.2020.9176254,33017951,,"{'text': 'The aim of this study was to design a new deep learning framework for end-to-end processing of polysomnograms. This framework can be trained to analyze whole-night polysomnograms without the limitations of and bias towards clinical scoring guidelines. We validated the framework by predicting the age of subjects. We designed a hierarchical attention network architecture, which can be pre-trained to predict labels based on 5-minute epochs of data and fine-tuned to predict based on whole-night polysomnography recordings. The model was trained on 511 recordings from the Cleveland Family study and tested on 146 test subjects aged between 6 to 88 years. The proposed network achieved a mean absolute error of 7.36 years and a correlation to true age of 0.857. Sleep can be analyzed using our end-to-end deep learning framework, which we expect can generalize to learning other subject-specific labels such as sleep disorders. The difference in the predicted and chronological age is further proposed as an estimate of biological age.', 'structured': []}",0,pubmed
Exactly conservative physics-informed neural networks and deep operator networks for dynamical systems.,"Elsa Cardoso-Bihlo (Department of Mathematics and Statistics Memorial University of Newfoundland St. John's, NL, A1C 5S7, Canada. Electronic address: ecardosobihl@mun.ca.); Alex Bihlo (Department of Mathematics and Statistics Memorial University of Newfoundland St. John's, NL, A1C 5S7, Canada. Electronic address: abihlo@mun.ca.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2024.106826,39509811,,"{'text': 'We introduce a method for training exactly conservative physics-informed neural networks and physics-informed deep operator networks for dynamical systems, that is, for ordinary differential equations. The method employs a projection-based technique that maps a candidate solution learned by the neural network solver for any given dynamical system possessing at least one first integral onto an invariant manifold. We illustrate that exactly conservative physics-informed neural network solvers and physics-informed deep operator networks for dynamical systems vastly outperform their non-conservative counterparts for several real-world problems from the mathematical sciences.', 'structured': []}",0,pubmed
On the approximation of functions by tanh neural networks.,"Tim De Ryck (Seminar for Applied Mathematics, ETH Zürich, Rämistrasse 101, 8092 Zürich, Switzerland. Electronic address: tim.deryck@sam.math.ethz.com.); Samuel Lanthaler (Seminar for Applied Mathematics, ETH Zürich, Rämistrasse 101, 8092 Zürich, Switzerland.); Siddhartha Mishra (Seminar for Applied Mathematics, ETH Zürich, Rämistrasse 101, 8092 Zürich, Switzerland.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2021.08.015,34482172,,"{'text': 'We derive bounds on the error, in high-order Sobolev norms, incurred in the approximation of Sobolev-regular as well as analytic functions by neural networks with the hyperbolic tangent activation function. These bounds provide explicit estimates on the approximation error with respect to the size of the neural networks. We show that tanh neural networks with only two hidden layers suffice to approximate functions at comparable or better rates than much deeper ReLU neural networks.', 'structured': []}",0,pubmed
Bidirectional deep recurrent neural networks for process fault classification.,"Gavneet Singh Chadha (Department of Automation Technology, South Westphalia University of Applied Sciences, Soest, Germany. Electronic address: chadha.gavneetsingh@fh-swf.de.); Ambarish Panambilly (Department of Automation Technology, South Westphalia University of Applied Sciences, Soest, Germany. Electronic address: panambilly.ambarish@fh-swf.de.); Andreas Schwung (Department of Automation Technology, South Westphalia University of Applied Sciences, Soest, Germany. Electronic address: schwung.andreas@fh-swf.de.); Steven X Ding (Department of Automatic Control and Complex Systems, University of Duisburg-Essen, Duisburg, Germany. Electronic address: steven.ding@uni-due.de.)",,ISA transactions,10.1016/j.isatra.2020.07.011,32684422,,"{'text': 'In this study, a new approach for time series based condition monitoring and fault diagnosis based on bidirectional recurrent neural networks is presented. The application of bidirectional recurrent neural networks essentially provide a viewpoint change on the fault diagnosis task, which allows to handle fault relations over longer time horizons helping in avoiding critical process breakdowns and increasing the overall productivity of the system. To further enhance the capability, we propose a novel procedure of data preprocessing and restructuring which enforces the generalization and a more efficient data utilization and consequently yields more efficient network training, especially for sequential fault classification task. The proposed Bidirectional Long Short Term Memory network outperforms standard recurrent architectures including vanilla recurrent neural networks, Long Short Term Memories and Gated Recurrent Units. We apply the proposed approach to the Tennessee Eastman benchmark process to test the effectiveness of the mentioned deep architectures and provide a detailed comparative analysis. The experimental results for binary as well as multi-class classification show the superior average fault detection capability of the bidirectional Long Short Term Memory Networks compared to the other architectures and to results from other state-of-the-art architectures found in the literature.', 'structured': []}",0,pubmed
Special issue on advances in neural networks research: IJCNN'07.,David G Brown (N/A); Jennie Si (N/A); Ron Sun (N/A),,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2008.01.002,18262753,,"{'text': '', 'structured': []}",0,pubmed
Quantum neural networks model based on swap test and phase estimation.,"Panchi Li (School of Computer and Information Technology, Northeast Petroleum University, Daqing 163318, China. Electronic address: lipanchi@vip.sina.com.); Bing Wang (School of Computer and Information Technology, Northeast Petroleum University, Daqing 163318, China.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2020.07.003,32663639,,"{'text': 'In this paper, a neural networks model for quantum computer is proposed. The core of this model is quantum neuron. Firstly, the inner product of the input qubits and the weight qubits is mapped to the phase of the control qubits in the neuron by the swap test technology, and then these phases are obtained by the phase estimation method, which are further used as the phase of the output qubit in the neuron. In this way, the mapping of input qubits to output qubit in quantum neuron is completed. The quantum neurons mentioned above can be used to construct quantum neural networks. In this paper, the quantum circuit for each operation step are given. The simulation results on the classic computer verify the effectiveness of the proposed model.', 'structured': []}",0,pubmed
On decision regions of narrow deep neural networks.,"Hans-Peter Beise (Department of Computer Science, Trier University of Applied Sciences, Germany. Electronic address: H.Beise@hochschule-trier.de.); Steve Dias Da Cruz (Department of Basics & Mathematical Models, IEE S.A., Luxembourg; Department of Computer Science, University of Kaiserslautern, Germany.); Udo Schröder (Department of Basics & Mathematical Models, IEE S.A., Luxembourg.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2021.02.024,33756267,,"{'text': 'We show that for neural network functions that have width less or equal to the input dimension all connected components of decision regions are unbounded. The result holds for continuous and strictly monotonic activation functions as well as for the ReLU activation function. This complements recent results on approximation capabilities by Hanin and Sellke (2017) and connectivity of decision regions by Nguyen et\xa0al. (2018) for such narrow neural networks. Our results are illustrated by means of numerical experiments.', 'structured': []}",0,pubmed
Efficient architecture for deep neural networks with heterogeneous sensitivity.,"Hyunjoong Cho (School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology (UNIST), Ulsan, Republic of Korea.); Jinhyeok Jang (Electronics and Telecommunications Research Institute (ETRI), Daejeon, Republic of Korea.); Chanhyeok Lee (School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology (UNIST), Ulsan, Republic of Korea.); Seungjoon Yang (School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology (UNIST), Ulsan, Republic of Korea. Electronic address: syang@unist.ac.kr.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2020.10.017,33302052,,"{'text': 'In this study, we present a neural network that consists of nodes with heterogeneous sensitivity. Each node in a network is assigned a variable that determines the sensitivity with which it learns to perform a given task. The network is trained via a constrained optimization that maximizes the sparsity of the sensitivity variables while ensuring optimal network performance. As a result, the network learns to perform a given task using only a few sensitive nodes. Insensitive nodes, which are nodes with zero sensitivity, can be removed from a trained network to obtain a computationally efficient network. Removing zero-sensitivity nodes has no effect on the performance of the network because the network has already been trained to perform the task without them. The regularization parameter used to solve the optimization problem was simultaneously found during the training of the networks. To validate our approach, we designed networks with computationally efficient architectures for various tasks such as autoregression, object recognition, facial expression recognition, and object detection using various datasets. In our experiments, the networks designed by our proposed method provided the same or higher performances but with far less computational complexity.', 'structured': []}",0,pubmed
Adaptive receptive field graph neural networks.,"Hepeng Gao (Jilin University, Changchun, 130012, Jilin, China. Electronic address: gaohepeng13@hotmail.com.); Funing Yang (Jilin University, Changchun, 130012, Jilin, China. Electronic address: harptomato@163.com.); Yongjian Yang (Jilin University, Changchun, 130012, Jilin, China. Electronic address: yyj@jlu.edu.cn.); Yuanbo Xu (Jilin University, Changchun, 130012, Jilin, China. Electronic address: yuanbox@jlu.edu.cn.); Yijun Su (JD Intelligent Cities Research, Beijing, 100007, China. Electronic address: suyijun.ucas@gmail.com.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2025.107658,40505162,,"{'text': ""Graph Neural Networks (GNNs) have drawn increasing attention in recent years and achieved outstanding success in many scenarios and tasks. However, existing methods indicate that the performance of representation learning drops dramatically as GNNs deepen, which is attributed to over-smoothing representation. To handle the above issue, we propose an adaptive receptive field graph neural network (ADRP-GNN) that aggregates information by adaptively expanding receptive fields with a monolayer graph convolution layer, avoiding deepening to result in the over-smoothing issue. Specifically, we first present a Multi-hop Graph Convolution Network (MuGC) that captures the information of the nodes and their multi-hop neighbors with only one layer, preventing frequent passing messages between nodes from the over-smoothing issue. Then, we design a Meta Learner that realizes the adaptive receptive field for each node to select related neighbor information. Finally, a Backbone Network is employed to enhance the architecture's learning ability. In addition, our architecture adaptively generates receptive fields instead of handcrafting stacked layers, which can integrate existing GNN frameworks to fit various scenarios. Extensive experiments indicate that our architecture is effective for the over-smoothing issue and improves accuracy by 0.52% to 6.88% compared to state-of-the-art methods on node classification tasks on eight datasets."", 'structured': []}",0,pubmed
Neural networks in analog hardware--design and implementation issues.,"S Draghici (Department of Computer Science, Wayne State University, Detroit, MI 48202, USA.)",,International journal of neural systems,10.1142/S0129065700000041,10798708,,"{'text': 'This paper presents a brief review of some analog hardware implementations of neural networks. Several criteria for the classification of general neural networks implementations are discussed and a taxonomy induced by these criteria is presented. The paper also discusses some characteristics of analog implementations as well as some trade-offs and issues identified in the work reviewed. Parameters such as precision, chip area, power consumption, speed and noise susceptibility are discussed in the context of neural implementations. A unified review of various ""VLSI friendly"" algorithms is also presented. The paper concludes with some conclusions drawn from the analysis of the implementations presented.', 'structured': []}",0,pubmed
3D Convolutional Neural Networks for Event-Related Potential detection.,H Cecotti (N/A); G Jha (N/A),,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,10.1109/EMBC.2019.8856485,31946786,,"{'text': 'Deep learning techniques have recently been successful in the classification of brain evoked responses for multiple applications, including brain-machine interface. Single-trial detection in the electroencephalogram (EEG) of brain evoked responses, like event-related potentials (ERPs), requires multiple processing stages, in the spatial and temporal domains, to extract high level features. Convolutional neural networks, as a type of deep learning method, have been used for EEG signal detection as the underlying structure of the EEG signal can be included in such system, facilitating the learning step. The EEG signal is typically decomposed into 2 main dimensions: space and time. However, the spatial dimension can be decomposed into 2 dimensions that better represent the relationships between the sensors that are involved in the classification. We propose to analyze the performance of 2D and 3D convolutional neural networks for the classification of ERPs with a dataset based on 64 EEG channels. We propose and compare 6 conv net architectures: 4 using 3D convolutions, that vary in relation to the number of layers and feature maps, and 2 using 2D convolutions. The results support the conclusion that 3D convolutions provide better performance than 2D convolutions for the binary classification of ERPs.', 'structured': []}",0,pubmed
Invited commentary: artificial neural networks--an introduction.,"M D Sawyer (Department of Surgery, Mayo Clinic, Rochester, Minn. 55905, USA.)",,Surgery,10.1067/msy.2000.102174,10660750,,"{'text': '', 'structured': []}",0,pubmed
Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective From the Time-Frequency Analysis.,Ce Ju (N/A); Cuntai Guan (N/A),,IEEE transactions on neural networks and learning systems,10.1109/TNNLS.2023.3307470,37725740,,"{'text': 'The motor imagery (MI) classification has been a prominent research topic in brain-computer interfaces (BCIs) based on electroencephalography (EEG). Over the past few decades, the performance of MI-EEG classifiers has seen gradual enhancement. In this study, we amplify the geometric deep-learning-based MI-EEG classifiers from the perspective of time-frequency analysis, introducing a new architecture called Graph-CSPNet. We refer to this category of classifiers as Geometric Classifiers, highlighting their foundation in differential geometry stemming from EEG spatial covariance matrices. Graph-CSPNet utilizes novel manifold-valued graph convolutional techniques to capture the EEG features in the time-frequency domain, offering heightened flexibility in signal segmentation for capturing localized fluctuations. To evaluate the effectiveness of Graph-CSPNet, we employ five commonly used publicly available MI-EEG datasets, achieving near-optimal classification accuracies in nine out of 11 scenarios. The Python repository can be found at https://github.com/GeometricBCI/Tensor-CSPNet-and-Graph-CSPNet.', 'structured': []}",0,pubmed
Transfer Learning with Deep Convolutional Neural Networks for Classifying Cellular Morphological Changes.,"Alexander Kensert (1 Department of Pharmaceutical Biosciences, Uppsala University, Uppsala, Sweden.); Philip J Harrison (1 Department of Pharmaceutical Biosciences, Uppsala University, Uppsala, Sweden.); Ola Spjuth (1 Department of Pharmaceutical Biosciences, Uppsala University, Uppsala, Sweden.)",,SLAS discovery : advancing life sciences R & D,10.1021/acs.jcim.8b00542.,30641024,,"{'text': 'The quantification and identification of cellular phenotypes from high-content microscopy images has proven to be very useful for understanding biological activity in response to different drug treatments. The traditional approach has been to use classical image analysis to quantify changes in cell morphology, which requires several nontrivial and independent analysis steps. Recently, convolutional neural networks have emerged as a compelling alternative, offering good predictive performance and the possibility to replace traditional workflows with a single network architecture. In this study, we applied the pretrained deep convolutional neural networks ResNet50, InceptionV3, and InceptionResnetV2 to predict cell mechanisms of action in response to chemical perturbations for two cell profiling datasets from the Broad Bioimage Benchmark Collection. These networks were pretrained on ImageNet, enabling much quicker model training. We obtain higher predictive accuracy than previously reported, between 95% and 97%. The ability to quickly and accurately distinguish between different cell morphologies from a scarce amount of labeled data illustrates the combined benefit of transfer learning and deep convolutional neural networks for interrogating cell-based images.', 'structured': []}",0,pubmed
Compact and Computationally Efficient Representation of Deep Neural Networks.,Simon Wiedemann (N/A); Klaus-Robert Muller (N/A); Wojciech Samek (N/A),,IEEE transactions on neural networks and learning systems,10.1109/TNNLS.2019.2910073,31150347,,"{'text': 'At the core of any inference procedure, deep neural networks are dot product operations, which are the component that requires the highest computational resources. For instance, deep neural networks, such as VGG-16, require up to 15-G operations in order to perform the dot products present in a single forward pass, which results in significant energy consumption and thus limits their use in resource-limited environments, e.g., on embedded devices or smartphones. One common approach to reduce the complexity of the inference is to prune and quantize the weight matrices of the neural network. Usually, this results in matrices whose entropy values are low, as measured relative to the empirical probability mass distribution of its elements. In order to efficiently exploit such matrices, one usually relies on, inter alia, sparse matrix representations. However, most of these common matrix storage formats make strong statistical assumptions about the distribution of the elements; therefore, cannot efficiently represent the entire set of matrices that exhibit low-entropy statistics (thus, the entire set of compressed neural network weight matrices). In this paper, we address this issue and present new efficient representations for matrices with low-entropy statistics. Alike sparse matrix data structures, these formats exploit the statistical properties of the data in order to reduce the size and execution complexity. Moreover, we show that the proposed data structures can not only be regarded as a generalization of sparse formats but are also more energy and time efficient under practically relevant assumptions. Finally, we test the storage requirements and execution performance of the proposed formats on compressed neural networks and compare them to dense and sparse representations. We experimentally show that we are able to attain up to ×42 compression ratios, ×5 speed ups, and ×90 energy savings when we lossless convert the state-of-the-art networks, such as AlexNet, VGG-16, ResNet152, and DenseNet, into the new data structures and benchmark their respective dot product.', 'structured': []}",0,pubmed
Regularization of deep neural networks with spectral dropout.,"Salman H Khan (Data61, Commonwealth Scientific and Industrial Research Organization (CSIRO), Canberra ACT 2601, Australia; The Australian National University, Canberra ACT 0200, Australia. Electronic address: salman.khan@anu.edu.au.); Munawar Hayat (University of Canberra, Bruce ACT 2617, Australia; Inception Institute of AI, Abu Dhabi, United Arab Emirates. Electronic address: munawar.hayat@canberra.edu.au.); Fatih Porikli (The Australian National University, Canberra ACT 0200, Australia. Electronic address: fatih.porikli@anu.edu.au.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2018.09.009,30504041,,"{'text': ""The big breakthrough on the ImageNet challenge in 2012 was partially due to the 'Dropout' technique used to avoid overfitting. Here, we introduce a new approach called 'Spectral Dropout' to improve the generalization ability of deep neural networks. We cast the proposed approach in the form of regular Convolutional Neural Network (CNN) weight layers using a decorrelation transform with fixed basis functions. Our spectral dropout method prevents overfitting by eliminating weak and 'noisy' Fourier domain coefficients of the neural network activations, leading to remarkably better results than the current regularization methods. Furthermore, the proposed is very efficient due to the fixed basis functions used for spectral transformation. In particular, compared to Dropout and Drop-Connect, our method significantly speeds up the network convergence rate during the training process (roughly ×2), with considerably higher neuron pruning rates (an increase of ∼30%). We demonstrate that the spectral dropout can also be used in conjunction with other regularization approaches resulting in additional performance gains."", 'structured': []}",0,pubmed
Bicomplex Projection Rule for Complex-Valued Hopfield Neural Networks.,"Masaki Kobayashi (Mathematical Science Center, University of Yamanashi, Kofu, Yamanashi 400-8511, Japan k-masaki@yamanashi.ac.jp.)",,Neural computation,10.1162/neco_a_01320,32946711,,"{'text': 'A complex-valued Hopfield neural network (CHNN) with a multistate activation function is a multistate model of neural associative memory. The weight parameters need a lot of memory resources. Twin-multistate activation functions were introduced to quaternion- and bicomplex-valued Hopfield neural networks. Since their architectures are much more complicated than that of CHNN, the architecture should be simplified. In this work, the number of weight parameters is reduced by bicomplex projection rule for CHNNs, which is given by the decomposition of bicomplex-valued Hopfield neural networks. Computer simulations support that the noise tolerance of CHNN with a bicomplex projection rule is equal to or even better than that of quaternion- and bicomplex-valued Hopfield neural networks. By computer simulations, we find that the projection rule for hyperbolic-valued Hopfield neural networks in synchronous mode maintains a high noise tolerance.', 'structured': []}",0,pubmed
Signal2Image Modules in Deep Neural Networks for EEG Classification.,Paschalis Bizopoulos (N/A); George I Lambrou (N/A); Dimitrios Koutsouris (N/A),,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,10.1109/EMBC.2019.8856620,31945994,,"{'text': ""Deep learning has revolutionized computer vision utilizing the increased availability of big data and the power of parallel computational units such as graphical processing units. The vast majority of deep learning research is conducted using images as training data, however the biomedical domain is rich in physiological signals that are used for diagnosis and prediction problems. It is still an open research question how to best utilize signals to train deep neural networks.In this paper we define the term Signal2Image (S2Is) as trainable or non-trainable prefix modules that convert signals, such as Electroencephalography (EEG), to image-like representations making them suitable for training image-based deep neural networks defined as `base models'. We compare the accuracy and time performance of four S2Is (`signal as image', spectrogram, one and two layer Convolutional Neural Networks (CNNs)) combined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet) along with the depth-wise and 1D variations of the latter. We also provide empirical evidence that the one layer CNN S2I performs better in eleven out of fifteen tested models than non-trainable S2Is for classifying EEG signals and present visual comparisons of the outputs of some of the S2Is."", 'structured': []}",0,pubmed
Hyperbolic Hopfield neural networks.,M Kobayashi (N/A),,IEEE transactions on neural networks and learning systems,10.1109/TNNLS.2012.2230450,24808287,,"{'text': 'In recent years, several neural networks using Clifford algebra have been studied. Clifford algebra is also called geometric algebra. Complex-valued Hopfield neural networks (CHNNs) are the most popular neural networks using Clifford algebra. The aim of this brief is to construct hyperbolic HNNs (HHNNs) as an analog of CHNNs. Hyperbolic algebra is a Clifford algebra based on Lorentzian geometry. In this brief, a hyperbolic neuron is defined in a manner analogous to a phasor neuron, which is a typical complex-valued neuron model. HHNNs share common concepts with CHNNs, such as the angle and energy. However, HHNNs and CHNNs are different in several aspects. The states of hyperbolic neurons do not form a circle, and, therefore, the start and end states are not identical. In the quantized version, unlike complex-valued neurons, hyperbolic neurons have an infinite number of states.', 'structured': []}",0,pubmed
Approximation rates for neural networks with general activation functions.,"Jonathan W Siegel (Department of Mathematics, Pennsylvania State University, University Park, PA 16802, USA. Electronic address: jus1949@psu.edu.); Jinchao Xu (Department of Mathematics, Pennsylvania State University, University Park, PA 16802, USA.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2020.05.019,32470796,,"{'text': 'We prove some new results concerning the approximation rate of neural networks with general activation functions. Our first result concerns the rate of approximation of a two layer neural network with a polynomially-decaying non-sigmoidal activation function. We extend the dimension independent approximation rates previously obtained to this new class of activation functions. Our second result gives a weaker, but still dimension independent, approximation rate for a larger class of activation functions, removing the polynomial decay assumption. This result applies to any bounded, integrable activation function. Finally, we show that a stratified sampling approach can be used to improve the approximation rate for polynomially decaying activation functions under mild additional assumptions.', 'structured': []}",0,pubmed
Heterophilous distribution propagation for Graph Neural Networks.,"Zhuonan Zheng (College of Computer Science, Zhejiang University, Hangzhou, 310027, China; Zhejiang Key Laboratory of Accessible Perception and Intelligent Systems, Zhejiang University, Hangzhou, 310027, China. Electronic address: zhengzn@zju.edu.cn.); Sheng Zhou (Zhejiang Key Laboratory of Accessible Perception and Intelligent Systems, Zhejiang University, Hangzhou, 310027, China; School of Software Technology, Zhejiang University, Ningbo, 315048, China. Electronic address: zhousheng_zju@zju.edu.cn.); Hongjia Xu (College of Computer Science, Zhejiang University, Hangzhou, 310027, China; Zhejiang Key Laboratory of Accessible Perception and Intelligent Systems, Zhejiang University, Hangzhou, 310027, China. Electronic address: xu_hj@zju.edu.cn.); Ming Gu (College of Computer Science, Zhejiang University, Hangzhou, 310027, China; Zhejiang Key Laboratory of Accessible Perception and Intelligent Systems, Zhejiang University, Hangzhou, 310027, China. Electronic address: guming444@zju.edu.cn.); Yilun Xu (College of Computer Science, Zhejiang University, Hangzhou, 310027, China; Zhejiang Key Laboratory of Accessible Perception and Intelligent Systems, Zhejiang University, Hangzhou, 310027, China. Electronic address: yilun.xu@zju.edu.cn.); Ao Li (Alibaba Group, Hangzhou, 310052, China. Electronic address: jianzhen.la@alibaba-inc.com.); Yuhong Li (Alibaba Group, Hangzhou, 310052, China. Electronic address: daniel.lyh@alibaba-inc.com.); Jingjun Gu (College of Computer Science, Zhejiang University, Hangzhou, 310027, China; Zhejiang Key Laboratory of Accessible Perception and Intelligent Systems, Zhejiang University, Hangzhou, 310027, China. Electronic address: gjj@zju.edu.cn.); Jiajun Bu (College of Computer Science, Zhejiang University, Hangzhou, 310027, China; Zhejiang Key Laboratory of Accessible Perception and Intelligent Systems, Zhejiang University, Hangzhou, 310027, China. Electronic address: bjj@zju.edu.cn.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2024.107014,39733698,,"{'text': 'Graph Neural Networks (GNNs) have achieved remarkable success in various graph mining tasks by aggregating information from neighborhoods for representation learning. The success relies on the homophily assumption that nearby nodes exhibit similar behaviors, while it may be violated in many real-world graphs. Recently, heterophilous graph neural networks (HeterGNNs) have attracted increasing attention by modifying the neural message passing schema for heterophilous neighborhoods. However, they suffer from insufficient neighborhood partition and heterophily modeling, both of which are critical but challenging to break through. To tackle these challenges, in this paper, we propose heterophilous distribution propagation (HDP) for graph neural networks. Instead of aggregating information from all neighborhoods, HDP adaptively separates the neighbors into homophilous and heterophilous parts based on the pseudo assignments during training. The heterophilous neighborhood distribution is learned with orthogonality-oriented constraint via a trusted prototype contrastive learning paradigm. Both the homophilous and heterophilous patterns are propagated with a novel semantic-aware message-passing mechanism. We conduct extensive experiments on 9 benchmark datasets with different levels of homophily. Experimental results show that our method outperforms representative baselines on heterophilous datasets.', 'structured': []}",0,pubmed
Binarized Simplicial Convolutional Neural Networks.,"Yi Yan (Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China. Electronic address: yiyiyiyan@outlook.com.); Ercan Engin Kuruoglu (Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, China. Electronic address: kuruoglu@sz.tsinghua.edu.cn.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2024.106928,39616932,,"{'text': 'Graph Neural Networks have the limitation of processing features solely on graph nodes, neglecting data on high-dimensional structures such as edges and triangles. Simplicial Convolutional Neural Networks (SCNN) represent high-order structures using simplicial complexes to break this limitation but still lack time efficiency. In this paper, a novel neural network architecture named Binarized Simplicial Convolutional Neural Networks (Bi-SCNN) is proposed based on the combination of simplicial convolution with a weighted binary-sign forward propagation strategy. The utilization of the Hodge Laplacian on a weighted binary-sign forward propagation enables Bi-SCNN to efficiently and effectively represent simplicial features with higher-order structures, surpassing the capabilities of traditional graph node representations. The Bi-SCNN achieves reduced model complexity compared to previous SSCN variants through binarization and normalization, also serving as intrinsic nonlinearities of Bi-SCNN; this enables Bi-SCNN to shorten the execution time without compromising prediction performance and makes Bi-SCNN less prone to over-smoothing. Experimenting with real-world citation and ocean-drifter data confirmed that our proposed Bi-SCNN is efficient and accurate.', 'structured': []}",0,pubmed
Task-dependent color representation in convolutional neural networks.,Jenny M Bosten (N/A); S Angela Diyalagoda (N/A),,"Journal of the Optical Society of America. A, Optics, image science, and vision",10.1364/JOSAA.546067,40793550,,"{'text': 'Convolutional neural networks (CNNs) trained for image categorization are known to have color-selective units that are tuned to particular colors. Analogously to the analysis of color representations in humans using brain imaging data, the representation of color within layers of a trained network can be characterized by constructing representational dissimilarity matrices (RDMs) and by using multidimensional scaling (MDS) to visualize geometric representational color spaces. Human color representations show flexibility dependent on the task. We trained CNNs on a set of simple chromatic stimuli but varied the ""task"" to require either color categorization, an analog of a color appearance rating, or luminance or spatial judgments that may not require color at all. We found that color representations within trained networks differed reliably and distinctively between task conditions, and that structured representations developed for color-relevant training conditions that were appropriate to the task. Color representations for different task conditions diverged through network layers toward the output layer, but they were significantly different even for layer 2 near the input layer. The variance between network instances was lowest for color-relevant tasks. For two of the tasks initially assumed to be ""color-irrelevant,"" reliable and distinctive color representations developed. For these tasks requiring luminance or spatial judgments, color must provide a useful cue even though it is not required in the network output. These results or similar models may be used to generate hypotheses for how color representations in the human brain vary with task (whether tasks obviously require color or not), which could be tested using neuroimaging.', 'structured': []}",0,pubmed
Problem solving in artificial neural networks.,"S Hampson (Department of Information and Computing, University of California, Irvine 92717.)",,Progress in neurobiology,10.1016/0301-0082(94)90065-5,8008826,,"{'text': '', 'structured': []}",0,pubmed
Projective Synchronization Analysis of Fractional-Order Neural Networks With Mixed Time Delays.,Peng Liu (N/A); Minxue Kong (N/A); Zhigang Zeng (N/A),,IEEE transactions on cybernetics,10.1109/TCYB.2020.3027755,33119534,,"{'text': 'In this article, we analyze the projective synchronization of fractional-order neural networks with mixed time delays. By introducing an extended Halanay inequality that is applicable for the case of fractional differential equations with arbitrary initial time and multiple types of delays, sufficient criteria are deduced for ensuring the projective synchronization of fractional-order neural networks with both discrete time-varying delays and distributed delays. Furthermore, sufficient criteria are presented for ensuring the projective synchronization in the Mittag-Leffler sense if there is no delay in fractional-order neural networks. The results derived herein include complete synchronization, anti-synchronization, and stabilization of fractional-order neural networks as particular cases. Moreover, the testable criteria in this article are a meaningful extension of projective synchronization of neural networks with mixed time delays from integer-order to fractional-order ones. A numerical simulation with four cases is provided to verify the validity of the obtained results.', 'structured': []}",0,pubmed
Heterogeneous graph neural networks enhance pressure estimation in water distribution networks.,"Jian Wang (Centre for Water Systems, University of Exeter, Exeter EX4 4QF, United Kingdom.); Li Liu (School of Civil Engineering, Hefei University of Technology, Hefei 230009, China.); Dragan Savic (Centre for Water Systems, University of Exeter, Exeter EX4 4QF, United Kingdom; KWR Water Research Institute, Niuwegein 3430 BB, the Netherlands.); Guangtao Fu (Centre for Water Systems, University of Exeter, Exeter EX4 4QF, United Kingdom. Electronic address: g.fu@exeter.ac.uk.)",,Water research,10.1016/j.watres.2025.123843,40424926,,"{'text': 'Pressure estimation is crucial for efficient operation and management of water distribution networks (WDNs). However, it is often challenged by limited sensor observations. While graph neural networks (GNNs) have been used to improve hydraulic and water quality predictions of WDNs, their reliance on homogeneous graphs oversimplifies the diverse roles and interactions of hydraulic components, resulting in lower performance under dynamic system states. This research introduces a novel heterogeneous graph neural network (HGNN) framework, which models control units such as pumps and valves as distinct nodes while preserving their interactions through additional edge types. Experimental results using C-Town as a benchmark demonstrate that HGNN outperforms GNN in terms of accuracy, robustness, and adaptability, achieving a mean absolute percentage error (MAPE) of 1.88 % and a mean absolute error (MAE) of 1.70 m under a 95 % masking rate. Additionally, this study shows that optimal sensor placement reduces MAE by up to 15 %, and the proposed HGNN framework achieves high computational efficiency, highlighting its effectiveness in WDN analysis and management. This research offers an advanced and transferable approach for WDN pressure estimation, serving as a superior alternative to traditional pressure evaluation models.', 'structured': []}",0,pubmed
Deep-gKnock: Nonlinear group-feature selection with deep neural networks.,"Guangyu Zhu (Department of Computer Science and Statistics, University of Rhode Island, United States of America. Electronic address: guangyuzhu@uri.edu.); Tingting Zhao (Department of Electrical and Computer Engineering, Northeastern University, United States of America.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2020.12.004,33385830,,"{'text': 'Feature selection is central to contemporary high-dimensional data analysis. Group structure among features arises naturally in various scientific problems. Many methods have been proposed to incorporate the group structure information into feature selection. However, these methods are normally restricted to a linear regression setting. To relax the linear constraint, we design a new Deep Neural Network (DNN) architecture and integrating it with the recently proposed knockoff technique to perform nonlinear group-feature selection with controlled group-wise False Discovery Rate (gFDR). Experimental results on high-dimensional synthetic data demonstrate that our method achieves the highest power and accurate gFDR control compared with state-of-the-art methods. The performance of Deep-gKnock is especially superior in the following five situations: (1) nonlinearity relationship; (2) dimension p greater than sample size n; (3) high between-group correlation; (4) high within-group correlation; (5) large number of associated groups. And Deep-gKnock is also demonstrated to be robust to the misspecification of the feature distribution and the change of network architecture. Moreover, Deep-gKnock achieves scientifically meaningful group-feature selection results for cutting-edge real world datasets.', 'structured': []}",0,pubmed
Multi-task Learning Graph Neural Networks for Cancer Prognosis Prediction with Genomic Data.,Tsung-Wei Lin (N/A); Sofia Ormazabal Arriagada (N/A); Che Lin (N/A),,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,10.1109/EMBC53108.2024.10782177,40039031,,"{'text': 'Providing robust prognosis predictions for cancers with limited data samples remains a challenge for precision oncology. In this study, we propose a novel approach that combines multi-task learning (MTL) and graph neural networks (GNNs) to address this issue. By representing gene-gene interactions as a graph network, our approach leverages multi-task learning to effectively capture the relationships of genes relevant to the oncogenesis and progression of breast, lung, and colon cancer. We demonstrate that our approach improves the cancer prognosis prediction for cancers with fewer samples, such as colon adenocarcinoma, by leveraging the shared gene-gene interactions across different cancer types, obtaining increases in the area under the precision-recall curve (AUPRC) of 24%. Our work contributes to the field of smart healthcare by demonstrating the potential of MTL and GNNs for enhancing cancer prognosis prediction, even with limited data samples.', 'structured': []}",0,pubmed
Designing Interpretable Recurrent Neural Networks for Video Reconstruction via Deep Unfolding.,Huynh Van Luong (N/A); Boris Joukovsky (N/A); Nikos Deligiannis (N/A),,IEEE transactions on image processing : a publication of the IEEE Signal Processing Society,10.1109/TIP.2021.3069296,33798083,,"{'text': ""Deep unfolding methods design deep neural networks as learned variations of optimization algorithms through the unrolling of their iterations. These networks have been shown to achieve faster convergence and higher accuracy than the original optimization methods. In this line of research, this paper presents novel interpretable deep recurrent neural networks (RNNs), designed by the unfolding of iterative algorithms that solve the task of sequential signal reconstruction (in particular, video reconstruction). The proposed networks are designed by accounting that video frames' patches have a sparse representation and the temporal difference between consecutive representations is also sparse. Specifically, we design an interpretable deep RNN (coined reweighted-RNN) by unrolling the iterations of a proximal method that solves a reweighted version of the l"", 'structured': []}",0,pubmed
Neural networks: what are they?,"K E Frye (University of South Alabama Medical Center, Mobile, USA.); S D Izenberg (N/A); M D Williams (N/A); A Luterman (N/A)",,The Journal of burn care & rehabilitation,10.1097/00004630-199701000-00013,9063792,,"{'text': '', 'structured': []}",0,pubmed
Deep neural-kernel blocks.,"Siamak Mehrkanoon (Department of Data Science and Knowledge Engineering, Maastricht University, The Netherlands. Electronic address: siamak.mehrkanoon@maastrichtuniversity.nl.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2019.03.011,31005850,,"{'text': 'This paper introduces novel deep architectures using the hybrid neural-kernel core model as the first building block. The proposed models follow a combination of a neural networks based architecture and a kernel based model enriched with pooling layers. In particular, in this context three kernel blocks with average, maxout and convolutional pooling layers are introduced and examined. We start with a simple merging layer which averages the output of the previous representation layers. The maxout layer on the other hand triggers competition among different representations of the input. Thanks to this pooling layer, not only the dimensionality of the output of multi-scale representations is reduced but also multiple sub-networks are formed within the same model. In the same context, the pointwise convolutional layer is also employed with the aim of projecting the multi-scale representations onto a new space. Experimental results show an improvement over the core deep hybrid model as well as kernel based models on several real-life datasets.', 'structured': []}",0,pubmed
Bridged adversarial training.,"Hoki Kim (Institute of Engineering Research, Seoul National University, Gwanak-gu 08826, Republic of Korea.); Woojin Lee (School of AI Convergence, Dongguk University-Seoul, Jung-gu 04620, Republic of Korea.); Sungyoon Lee (Department of Computer Science, Hanyang University, Seongdong-gu 04763, Republic of Korea.); Jaewook Lee (Department of Industrial Engineering, Seoul National University, Gwanak-gu 08826, Republic of Korea. Electronic address: jaewook@snu.ac.kr.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2023.08.024,37666185,,"{'text': 'Adversarial robustness is considered a required property of deep neural networks. In this study, we discover that adversarially trained models might have significantly different characteristics in terms of margin and smoothness, even though they show similar robustness. Inspired by the observation, we investigate the effect of different regularizers and discover the negative effect of the smoothness regularizer on maximizing the margin. Based on the analyses, we propose a new method called bridged adversarial training that mitigates the negative effect by bridging the gap between clean and adversarial examples. We provide theoretical and empirical evidence that the proposed method provides stable and better robustness, especially for large perturbations.', 'structured': []}",0,pubmed
Fixed-deviation stabilization and synchronization for delayed fractional-order complex-valued neural networks.,"Bingrui Zhang (School of Mathematics and Statistics, Hubei Normal University, Huangshi 435002, China.); Jin-E Zhang (School of Mathematics and Statistics, Hubei Normal University, Huangshi 435002, China.)",,Mathematical biosciences and engineering : MBE,10.3934/mbe.2023449,37322931,,"{'text': 'In this paper, we study fixed-deviation stabilization and synchronization for fractional-order complex-valued neural networks with delays. By applying fractional calculus and fixed-deviation stability theory, sufficient conditions are given to ensure the fixed-deviation stabilization and synchronization for fractional-order complex-valued neural networks under the linear discontinuous controller. Finally, two simulation examples are presented to show the validity of theoretical results.', 'structured': []}",0,pubmed
Combining Deep Learning Neural Networks with Genetic Algorithms to Map Nanocluster Configuration Spaces with Quantum Accuracy at Low Computational Cost.,"Johnathan von der Heyde (Department of Physics, University of Central Florida, 4000 Central Florida Blvd., Orlando, Florida 32816, United States.); Walter Malone (Department of Physics, Tuskegee University, 1200 W. Montgomery Rd., Tuskegee, Alabama 36088, United States.); Nusaiba Zaman (Department of Physics, University of Central Florida, 4000 Central Florida Blvd., Orlando, Florida 32816, United States.); Abdelkader Kara (Department of Physics, University of Central Florida, 4000 Central Florida Blvd., Orlando, Florida 32816, United States.)",,Journal of chemical information and modeling,10.1021/acs.jcim.3c00609,37579032,,"{'text': ""The configuration spaces for bimetallic AuPd nanoclusters of various sizes are explored efficiently and analyzed accurately by combining genetic algorithms with neural networks trained on density functional theory. The methodology demonstrated herein provides an optimizable solution to the problem of searching vast configuration spaces with quantum accuracy in a way that is computationally practical. We implement a machine learning algorithm which learns the density functional theory potential with increasing performance while simultaneously generating and relaxing structures within the system's global configuration space unbiasedly. As a result, the algorithm naturally converges onto the system's energy minima while mapping the configuration space as a function of energy. The algorithm's simple design applies not only to nanocluster configurations, as demonstrated, but to bulk, substrate, and adsorption sites as well, and it is designed to scale. To demonstrate its computational efficiency, we work with AuPd nanoclusters of sizes 15, 20, and 25 atoms. Results focus primarily on evaluating the algorithm's performance; however, several physical insights into possible configurations for these nanoclusters naturally emerge as well, such as geometric Au surface segregation and stoichiometric Au minimization as a function of stability."", 'structured': []}",0,pubmed
An efficient deep learning approach to identify dynamics in in vitro neural networks.,Vito Paolo Pastore (N/A); Giulia Parodi (N/A); Martina Brofiga (N/A); Paolo Massobrio (N/A); Michela Chiappalone (N/A); Francesca Odone (N/A); Sergio Martinoia (N/A),,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,10.1109/EMBC40787.2023.10340862,38083487,,"{'text': 'Understanding and discriminating the spatiotemporal patterns of activity generated by in vitro and in vivo neuronal networks is a fundamental task in neuroscience and neuroengineering. The state-of-the-art algorithms to describe the neuronal activity mostly rely on global and local well-established spike and burst-related parameters. However, they are not able to capture slight differences in the activity patterns. In this work, we introduce a deep-learning-based algorithm to automatically infer the dynamics exhibited by different neuronal populations. Specifically, we demonstrate that our algorithm is able to discriminate with high accuracy the dynamics of five different populations of in vitro human-derived neural networks with an increasing inhibitory to excitatory neurons ratio.', 'structured': []}",0,pubmed
Deep Learning Adapted to Differential Neural Networks Used as Pattern Classification of Electrophysiological Signals.,D Llorente-Vidrio (N/A); M Ballesteros (N/A); I Salgado (N/A); I Chairez (N/A),,IEEE transactions on pattern analysis and machine intelligence,10.1109/TPAMI.2021.3066996,33735073,,"{'text': 'This manuscript presents the design of a deep differential neural network (DDNN) for pattern classification. First, we proposed a DDNN topology with three layers, whose learning laws are derived from a Lyapunov analysis, justifying local asymptotic convergence of the classification error and the weights of the DDNN. Then, an extension to include an arbitrary number of hidden layers in the DDNN is analyzed. The learning laws for this general form of the DDNN offer a contribution to the deep learning framework for signal classification with biological nature and dynamic structures. The DDNN is used to classify electroencephalographic signals from volunteers that perform an identification graphical test. The classification results show exponential growth in the signal classification accuracy from 82 percent with one layer to 100 percent with three hidden layers. Working with DDNN instead of static deep neural networks (SDNN) represents a set of advantages, such as processing time and training period reduction up to almost 100 times, and the increment of the classification accuracy while working with less hidden layers than working with SDNN, which are highly dependent on their topology and the number of neurons in each layer. The DDNN employed fewer neurons due to the induced feedback characteristic.', 'structured': []}",0,pubmed
Advancing Chest X-ray Diagnostics via Multi-Modal Neural Networks with Attention.,Douglas Townsell (N/A); Tanvi Banerjee (N/A); Lingwei Chen (N/A); Michael Raymer (N/A),,Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference,10.1109/EMBC53108.2024.10781551,40040114,,"{'text': ""The healthcare field is undergoing a profound shift, with deep learning in AI increasingly augmenting medical expertise in complex and challenging tasks. Our research addresses the challenging task of chest X-ray image diagnostics, a field characterized by multifaceted diagnostic labels and class im-balances in respiratory disease cases. Our approach synergizes a pre-trained image classification neural network with patient and image metadata integration, significantly boosting diagnostic precision. A key aspect of our methodology is the identification of an effective decision boundary to enhance accuracy and reduce false positives. The effectiveness of our model is demonstrated by an average AUC score of 0.84, surpassing existing models and signifying a notable leap in AI's role in medical diagnostics. This tool stands to aid clinical decision-making, particularly in navigating the complexities of comorbidities in respiratory health."", 'structured': []}",0,pubmed
Deep neural network representation and Generative Adversarial Learning.,"Ariel Ruiz-Garcia (SeeChange.ai, Manchester, United Kingdom. Electronic address: ariel.9arcia@gmail.com.); Jürgen Schmidhuber (NNAISENSE, Swiss AI Lab IDSIA, USI and SUPSI, Lugano, Switzerland. Electronic address: juergen@idsia.ch.); Vasile Palade (Centre for Data Science, Coventry University, Coventry, United Kingdom. Electronic address: vasile.palade@coventry.ac.uk.); Clive Cheong Took (Department of Electronic Engineering, Royal Holloway (University of London), Surrey, United Kingdom. Electronic address: Clive.CheongTook@rhul.ac.uk.); Danilo Mandic (Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom. Electronic address: d.mandic@imperial.ac.uk.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2021.03.009,33774356,,"{'text': '', 'structured': []}",0,pubmed
Universality of reservoir systems with recurrent neural networks.,"Hiroki Yasumoto (Graduate School of Informatics, Kyoto University, 36-1, Yoshida Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan. Electronic address: yasumoto@sys.i.kyoto-u.ac.jp.); Toshiyuki Tanaka (Graduate School of Informatics, Kyoto University, 36-1, Yoshida Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan. Electronic address: tt@i.kyoto-u.ac.jp.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2025.107413,40187082,,"{'text': 'Approximation capability of reservoir systems whose reservoir is a recurrent neural network (RNN) is discussed. We show what we call uniform strong universality of RNN reservoir systems for a certain class of dynamical systems. This means that, given an approximation error to be achieved, one can construct an RNN reservoir system that approximates each target dynamical system in the class just via adjusting its linear readout. To show the universality, we construct an RNN reservoir system via parallel concatenation that has an upper bound of approximation error independent of each target in the class.', 'structured': []}",0,pubmed
Modeling Chemical Reaction Networks Using Neural Ordinary Differential Equations.,"Anna C M Thöni (Donders Centre for Cognition, Radboud University, Nijmegen 9103 6500 HD, The Netherlands.); William E Robinson (Institute for Molecules and Materials, Radboud University, Nijmegen 9010 6500 GL, The Netherlands.); Yoram Bachrach (Meta FAIR, London N1C 4DB, United Kingdom.); Wilhelm T S Huck (Institute for Molecules and Materials, Radboud University, Nijmegen 9010 6500 GL, The Netherlands.); Tal Kachman (Donders Centre for Cognition, Radboud University, Nijmegen 9103 6500 HD, The Netherlands.)",,Journal of chemical information and modeling,10.1126/science.1165893,40262040,,"{'text': 'In chemical reaction network theory, ordinary differential equations are used to model the temporal change of chemical species concentration. As the functional form of these ordinary differential equation systems is derived from an empirical model of the reaction network, it may be incomplete. Our approach aims to elucidate these hidden insights in the reaction network by combining dynamic modeling with deep learning in the form of neural ordinary differential equations. Our contributions not only help to identify the shortcomings of existing empirical models but also assist the design of future reaction networks.', 'structured': []}",0,pubmed
A faster way to model neuronal circuitry.,"Andrew P Davison (Institut des Neurosciences Paris-Saclay, Université Paris-Saclay, CNRS, Saclay, France.); Shailesh Appukuttan (Institut des Neurosciences Paris-Saclay, Université Paris-Saclay, CNRS, Saclay, France.)",,eLife,10.3389/fncom.2021.800875,36458813,,"{'text': 'Artificial neural networks could pave the way for efficiently simulating large-scale models of neuronal networks in the nervous system.', 'structured': []}",0,pubmed
Prediction of mRNA subcellular localization using deep recurrent neural networks.,"Zichao Yan (School of Computer Science, McGill University, Montreal, QC, Canada.); Eric Lécuyer (Department of Biochemistry, University of Montreal, Montreal, QC, Canada.); Mathieu Blanchette (School of Computer Science, McGill University, Montreal, QC, Canada.)",,"Bioinformatics (Oxford, England)",10.1093/bioinformatics/btz337,31510698,,"{'text': 'Messenger RNA subcellular localization mechanisms play a crucial role in post-transcriptional gene regulation. This trafficking is mediated by trans-acting RNA-binding proteins interacting with cis-regulatory elements called zipcodes. While new sequencing-based technologies allow the high-throughput identification of RNAs localized to specific subcellular compartments, the precise mechanisms at play, and their dependency on specific sequence elements, remain poorly understood.', 'structured': [{'label': 'MOTIVATION', 'text': 'Messenger RNA subcellular localization mechanisms play a crucial role in post-transcriptional gene regulation. This trafficking is mediated by trans-acting RNA-binding proteins interacting with cis-regulatory elements called zipcodes. While new sequencing-based technologies allow the high-throughput identification of RNAs localized to specific subcellular compartments, the precise mechanisms at play, and their dependency on specific sequence elements, remain poorly understood.'}, {'label': 'RESULTS', 'text': ""We introduce RNATracker, a novel deep neural network built to predict, from their sequence alone, the distributions of mRNA transcripts over a predefined set of subcellular compartments. RNATracker integrates several state-of-the-art deep learning techniques (e.g. CNN, LSTM and attention layers) and can make use of both sequence and secondary structure information. We report on a variety of evaluations showing RNATracker's strong predictive power, which is significantly superior to a variety of baseline predictors. Despite its complexity, several aspects of the model can be isolated to yield valuable, testable mechanistic hypotheses, and to locate candidate zipcode sequences within transcripts.""}, {'label': 'AVAILABILITY AND IMPLEMENTATION', 'text': 'Code and data can be accessed at https://www.github.com/HarveyYan/RNATracker.'}, {'label': 'SUPPLEMENTARY INFORMATION', 'text': 'Supplementary data are available at Bioinformatics online.'}]}",0,pubmed
Distilling knowledge from graph neural networks trained on cell graphs to non-neural student models.,"Vasundhara Acharya (Rensselaer Polytechnic Institute, Troy, USA. acharv2@rpi.edu.); Bülent Yener (Professor, Rensselaer Polytechnic Institute, Troy, USA.); Gillian Beamer (Adjunct Associate Professor, Texas Biomedical Research Institute, San Antonio, TX, USA.)",,Scientific reports,10.1038/s41598-025-13697-7,40784939,,"{'text': ""The development and refinement of artificial intelligence (AI) and machine learning algorithms have been an area of intense research in radiology and pathology, particularly for automated or computer-aided diagnosis. Whole Slide Imaging (WSI) has emerged as a promising tool for developing and utilizing such algorithms in diagnostic and experimental pathology. However, patch-wise analysis of WSIs often falls short of capturing the intricate cell-level interactions within local microenvironment. A robust alternative to address this limitation involves leveraging cell graph representations, thereby enabling a more detailed analysis of local cell interactions. These cell graphs encapsulate the local spatial arrangement of cells in histopathology images, a factor proven to have significant prognostic value. Graph Neural Networks (GNNs) can effectively utilize these spatial feature representations and other features, demonstrating promising performance across classification tasks of varying complexities. It is also feasible to distill the knowledge acquired by deep neural networks to smaller student models through knowledge distillation (KD), achieving goals such as model compression and performance enhancement. Traditional approaches for constructing cell graphs generally rely on edge thresholds defined by sparsity/density or the assumption that nearby cells interact. However, such methods may fail to capture biologically meaningful interactions. Additionally, existing works in knowledge distillation primarily focus on distilling knowledge between neural networks. We designed cell graphs with biologically informed edge thresholds or criteria to address these limitations, moving beyond density/sparsity-based definitions. Furthermore, we demonstrated that student models do not need to be neural networks. Even non-neural models can learn from a neural network teacher. We evaluated our approach across varying dataset complexities, including the presence or absence of distribution shifts, varying degrees of imbalance, and different levels of graph complexity for training GNNs. We also investigated whether softened probabilities obtained from calibrated logits offered better guidance than raw logits. Our experiments revealed that the teacher's guidance was effective when distribution shifts existed in the data. The teacher model demonstrated decent performance due to its higher complexity and ability to use cell graph structures and features. Its logits provided rich information and regularization to students, mitigating the risk of overfitting the training distribution. We also examined the differences in feature importance between student models trained with the teacher's logits and their counterparts trained on hard labels. In particular, the student model demonstrated a stronger emphasis on morphological features in the Tuberculosis (TB) dataset than the models trained with hard labels. This emphasis aligns closely with the features that pathologists typically prioritize for diagnostic purposes. Future work could explore designing alternative teacher models, evaluating the proposed approach on larger datasets, and investigating causal knowledge distillation as a potential extension."", 'structured': []}",0,pubmed
Evolving Deep Neural Networks via Cooperative Coevolution With Backpropagation.,Maoguo Gong (N/A); Jia Liu (N/A); A K Qin (N/A); Kun Zhao (N/A); Kay Chen Tan (N/A),,IEEE transactions on neural networks and learning systems,10.1109/TNNLS.2020.2978857,32217489,,"{'text': ""Deep neural networks (DNNs), characterized by sophisticated architectures capable of learning a hierarchy of feature representations, have achieved remarkable successes in various applications. Learning DNN's parameters is a crucial but challenging task that is commonly resolved by using gradient-based backpropagation (BP) methods. However, BP-based methods suffer from severe initialization sensitivity and proneness to getting trapped into inferior local optima. To address these issues, we propose a DNN learning framework that hybridizes CC-based optimization with BP-based gradient descent, called BPCC, and implement it by devising a computationally efficient CC-based optimization technique dedicated to DNN parameter learning. In BPCC, BP will intermittently execute for multiple training epochs. Whenever the execution of BP in a training epoch cannot sufficiently decrease the training objective function value, CC will kick in to execute by using the parameter values derived by BP as the starting point. The best parameter values obtained by CC will act as the starting point of BP in its next training epoch. In CC-based optimization, the overall parameter learning task is decomposed into many subtasks of learning a small portion of parameters. These subtasks are individually addressed in a cooperative manner. In this article, we treat neurons as basic decomposition units. Furthermore, to reduce the computational cost, we devise a maturity-based subtask selection strategy to selectively solve some subtasks of higher priority. Experimental results demonstrate the superiority of the proposed method over common-practice DNN parameter learning techniques."", 'structured': []}",0,pubmed
Learning Student Networks via Feature Embedding.,Hanting Chen (N/A); Yunhe Wang (N/A); Chang Xu (N/A); Chao Xu (N/A); Dacheng Tao (N/A),,IEEE transactions on neural networks and learning systems,10.1109/TNNLS.2020.2970494,32092018,,"{'text': 'Deep convolutional neural networks have been widely used in numerous applications, but their demanding storage and computational resource requirements prevent their applications on mobile devices. Knowledge distillation aims to optimize a portable student network by taking the knowledge from a well-trained heavy teacher network. Traditional teacher-student-based methods used to rely on additional fully connected layers to bridge intermediate layers of teacher and student networks, which brings in a large number of auxiliary parameters. In contrast, this article aims to propagate information from teacher to student without introducing new variables that need to be optimized. We regard the teacher-student paradigm from a new perspective of feature embedding. By introducing the locality preserving loss, the student network is encouraged to generate the low-dimensional features that could inherit intrinsic properties of their corresponding high-dimensional features from the teacher network. The resulting portable network, thus, can naturally maintain the performance as that of the teacher network. Theoretical analysis is provided to justify the lower computation complexity of the proposed method. Experiments on benchmark data sets and well-trained networks suggest that the proposed algorithm is superior to state-of-the-art teacher-student learning methods in terms of computational and storage complexity.', 'structured': []}",0,pubmed
Process-Informed Neural Networks: A Hybrid Modelling Approach to Improve Predictive Performance and Inference of Neural Networks in Ecology and Beyond.,"Marieke Wesselkamp (Biometry and Environmental System Analysis, University of Freiburg, Freiburg im Breisgau, Germany.); Niklas Moser (Biometry and Environmental System Analysis, University of Freiburg, Freiburg im Breisgau, Germany.); Maria Kalweit (Department of Computer Science, University of Freiburg, Freiburg im Breisgau, Germany.); Joschka Boedecker (Department of Computer Science, University of Freiburg, Freiburg im Breisgau, Germany.); Carsten F Dormann (Biometry and Environmental System Analysis, University of Freiburg, Freiburg im Breisgau, Germany.)",,Ecology letters,10.1111/ele.70012,39625058,,"{'text': 'Despite deep learning being state of the art for data-driven model predictions, its application in ecology is currently subject to two important constraints: (i) deep-learning methods are powerful in data-rich regimes, but in ecology data are typically sparse; and (ii) deep-learning models are black-box methods and inferring the processes they represent are non-trivial to elicit. Process-based (=\u2009mechanistic) models are not constrained by data sparsity or unclear processes and are thus important for building up our ecological knowledge and transfer to applications. In this work, we combine process-based models and neural networks into process-informed neural networks (PINNs), which incorporate the process knowledge directly into the neural network structure. In a systematic evaluation of spatial and temporal prediction tasks for C-fluxes in temperate forests, we show the ability of five different types of PINNs (i) to outperform process-based models and neural networks, especially in data-sparse regimes with high-transfer task and (ii) to inform on mis- or undetected processes.', 'structured': []}",0,pubmed
Redundant feature pruning for accelerated inference in deep neural networks.,"Babajide O Ayinde (Electrical and Computer Engineering, University of Louisville, Louisville, KY, 40292, USA. Electronic address: babajide.ayinde@louisville.edu.); Tamer Inanc (Electrical and Computer Engineering, University of Louisville, Louisville, KY, 40292, USA. Electronic address: t.inanc@louisville.edu.); Jacek M Zurada (Electrical and Computer Engineering, University of Louisville, Louisville, KY, 40292, USA; Information Technology Institute, University of Social Science, Łódz 90-113, Poland. Electronic address: jacek.zurada@louisville.edu.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2019.04.021,31279285,,"{'text': 'This paper presents an efficient technique to reduce the inference cost of deep and/or wide convolutional neural network models by pruning redundant features (or filters). Previous studies have shown that over-sized deep neural network models tend to produce a lot of redundant features that are either shifted version of one another or are very similar and show little or no variations, thus resulting in filtering redundancy. We propose to prune these redundant features along with their related feature maps according to their relative cosine distances in the feature space, thus leading to smaller networks with reduced post-training inference computational costs and competitive performance. We empirically show on select models (VGG-16, ResNet-56, ResNet-110, and ResNet-34) and dataset (MNIST Handwritten digits, CIFAR-10, and ImageNet) that inference costs (in FLOPS) can be significantly reduced while overall performance is still competitive with the state-of-the-art.', 'structured': []}",0,pubmed
SympGNNs: Symplectic Graph Neural Networks for identifying high-dimensional Hamiltonian systems and node classification.,"Alan John Varghese (School of Engineering, Brown University, Providence, RI 02912, USA.); Zhen Zhang (Division of Applied Mathematics, Brown University, Providence, RI 02912, USA.); George Em Karniadakis (School of Engineering, Brown University, Providence, RI 02912, USA; Division of Applied Mathematics, Brown University, Providence, RI 02912, USA. Electronic address: george_karniadakis@brown.edu.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2025.107397,40127578,,"{'text': 'Existing neural network models to learn Hamiltonian systems, such as SympNets, although accurate in low-dimensions, struggle to learn the correct dynamics for high-dimensional many-body systems. Herein, we introduce Symplectic Graph Neural Networks (SympGNNs) that can effectively handle system identification in high-dimensional Hamiltonian systems, as well as node classification. SympGNNs combine symplectic maps with permutation equivariance, a property of graph neural networks. Specifically, we propose two variants of SympGNNs: (i) G-SympGNN and (ii) LA-SympGNN, arising from different parameterizations of the kinetic and potential energy. We demonstrate the capabilities of SympGNN on two physical examples: a 40-particle coupled Harmonic oscillator, and a 2000-particle molecular dynamics simulation in a two-dimensional Lennard-Jones potential. Furthermore, we demonstrate the performance of SympGNN in the node classification task, achieving accuracy comparable to the state-of-the-art. We also empirically show that SympGNN can overcome the oversmoothing and heterophily problems, two key challenges in the field of graph neural networks.', 'structured': []}",0,pubmed
Motif and supernode-enhanced gated graph neural networks for session-based recommendation.,"Ronghua Lin (School of Computer Science, South China Normal University, Guangzhou, 510631, China; Pazhou Lab, Guangzhou, 510330, China. Electronic address: rhlin@m.scnu.edu.cn.); Chang Liu (School of Computer Science, South China Normal University, Guangzhou, 510631, China. Electronic address: liuchang6780@m.scnu.edu.cn.); Hao Zhong (School of Computer Science, South China Normal University, Guangzhou, 510631, China; Pazhou Lab, Guangzhou, 510330, China. Electronic address: hzhong@m.scnu.edu.cn.); Chengzhe Yuan (School of Electronics and Information, Guangdong Polytechnic Normal University, Guangzhou, 510665, China; Pazhou Lab, Guangzhou, 510330, China. Electronic address: ycz@gpnu.edu.cn.); Guohua Chen (School of Computer Science, South China Normal University, Guangzhou, 510631, China; Pazhou Lab, Guangzhou, 510330, China. Electronic address: chengh@m.scnu.edu.cn.); Yuncheng Jiang (School of Computer Science, South China Normal University, Guangzhou, 510631, China. Electronic address: jiangyuncheng@m.scnu.edu.cn.); Yong Tang (School of Computer Science, South China Normal University, Guangzhou, 510631, China; Pazhou Lab, Guangzhou, 510330, China. Electronic address: ytang@m.scnu.edu.cn.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2025.107406,40147160,,"{'text': ""Session-based recommendation systems aim to predict users' next interactions based on short-lived, anonymous sessions, a challenging yet vital task due to the sparsity and dynamic nature of user behavior. Existing Graph Neural Network (GNN)-based methods primarily focus on the session graphs while overlooking the influence of micro-structures and user behavior patterns. To address these limitations, we propose a Motif and Supernode-Enhanced Session-based Recommender System (MSERS), which constructs a global session graph, identifies and encodes motifs as supernodes, and reintegrates them into the global graph to enrich its topology and better represent item dependencies. By employing supernode-enhanced Gated Graph Neural Networks (GGNN), MSERS captures both long-term and latent item dependencies, significantly improving session representations. Extensive experiments on two real-world datasets demonstrate the superiority of MSERS over baseline methods, providing robust insights into the role of micro-structures in session-based recommendations."", 'structured': []}",0,pubmed
Deep learning in terrestrial conservation biology.,"Zoltán Barta (HUN-REN-DE Behavioural Ecology Research Group, Department of Evolutionary Zoology and Humanbiology, University of Debrecen, Debrecen, Hungary. barta.zoltan@science.unideb.hu.)",,Biologia futura,10.1111/cobi.13616,38227170,,"{'text': 'Biodiversity is being lost at an unprecedented rate on Earth. As a first step to more effectively combat this process we need efficient methods to monitor biodiversity changes. Recent technological advance can provide powerful tools (e.g.\xa0camera traps, digital acoustic recorders, satellite imagery, social media records) that can speed up the collection of biological data. Nevertheless, the processing steps of the raw data served by these tools are still painstakingly slow. A new computer technology, deep learning based artificial intelligence, might, however, help. In this short and subjective review I oversee recent technological advances used in conservation biology, highlight problems of processing their data, shortly describe deep learning technology and show case studies of its use in conservation biology. Some of the limitations of the technology are also highlighted.', 'structured': []}",0,pubmed
Advancing EEG based stress detection using spiking neural networks and convolutional spiking neural networks.,"Aaditya Joshi (VIT Bhopal University, Sehore, Madhya Pradesh, India.); Paramveer Singh Matharu (VIT Bhopal University, Sehore, Madhya Pradesh, India.); Lokesh Malviya (VIT Bhopal University, Sehore, Madhya Pradesh, India.); Manoj Kumar (VIT Bhopal University, Sehore, Madhya Pradesh, India.); Akshay Jadhav (Manipal University Jaipur, Jaipur, Rajasthan, India. akshay.jadhav@jaipur.manipal.edu.)",,Scientific reports,10.1038/s41598-025-10270-0,40683976,,"{'text': 'Accurate and efficient analysis of Electroencephalogram (EEG) signals is crucial for applications like neurological diagnosis and Brain-Computer Interfaces (BCI). Traditional methods often fall short in capturing the intricate temporal dynamics inherent in EEG data. This paper explores the use of Convolutional Spiking Neural Networks (CSNNs) to enhance EEG signal classification. We apply Discrete Wavelet Transform (DWT) for feature extraction and evaluate CSNN performance on the Physionet EEG dataset, benchmarking it against traditional deep learning and machine learning methods. The findings indicate that CSNNs achieve high accuracy, reaching 98.75% in 10-fold cross-validation, and an impressive F1 score of 98.60%. Notably, this F1-score represents an improvement over previous benchmarks, highlighting the effectiveness of our approach. Along with offering advantages in temporal precision and energy efficiency, CSNNs emerge as a promising solution for next-generation EEG analysis systems.', 'structured': []}",0,pubmed
Synchronization criteria for quaternion-valued coupled neural networks with impulses.,"Xingnan Qi (School of Mathematics and Statistics, Southwest University, Chongqing 400715, China.); Haibo Bao (School of Mathematics and Statistics, Southwest University, Chongqing 400715, China. Electronic address: hbbao@swu.edu.cn.); Jinde Cao (School of Mathematics, Southeast University, Nanjing 210096, China.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2020.04.027,32446192,,"{'text': 'We consider the global exponential synchronization of a category of quaternion-valued coupled neural networks (QVCNNs) with impulses in this article. It makes up for the gap of coupled neural networks with impulses in quaternion. On account of the product of two quaternions cannot be exchanged under normal circumstances, for convenience, we isolate the QVCNN into four real-valued coupled neural networks (RVCNNs) which are converted into an augmented system by defining a new augmented vector. By leveraging a distinctive Lyapunov-Krasovskii function and some matrix inequalities, several sufficient conditions for the global exponential synchronization of the system are attained. Ultimately, two examples are used to prove the validity of the theories in this paper.', 'structured': []}",0,pubmed
Label-Aware Dual Graph Neural Networks for Multi-Label Fundus Image Classification.,Yanbei Liu (N/A); Xinwen Peng (N/A); Xin Wei (N/A); Lei Geng (N/A); Fang Zhang (N/A); Zhitao Xiao (N/A); Jerry Chun-Wei Lin (N/A),,IEEE journal of biomedical and health informatics,10.1109/JBHI.2024.3457232,39255075,,"{'text': ""Fundus disease is a complex and universal disease involving a variety of pathologies. Its early diagnosis using fundus images can effectively prevent further diseases and provide targeted treatment plans for patients. Recent deep learning models for classification of this disease are gradually emerging as a critical research field, which is attracting widespread attention. However, in practice, most of the existing methods only focus on local visual cues of a single image, and ignore the underlying explicit interaction similarity between subjects and correlation information among pathologies in fundus diseases. In this paper, we propose a novel label-aware dual graph neural networks for multi-label fundus image classification that consists of population-based graph representation learning and pathology-based graph representation learning modules. Specifically, we first construct a population-based graph by integrating image features and non-image information to learn patient's representations by incorporating associations between subjects. Then, we represent pathologies as a sparse graph where its nodes are associated with pathology-based feature vectors and the edges correspond to probability of the co-occurrence of labels to generate a set of classifier scores by the propagation of multi-layer graph information. Finally, our model can adaptively recalibrate multi-label outputs. Detailed experiments and analysis of our results show the effectiveness of our method compared with state-of-the-art multi-label fundus image classification methods."", 'structured': []}",0,pubmed
"If deep learning is the answer, what is the question?","Andrew Saxe (Department of Experimental Psychology, University of Oxford, Oxford, UK. andrew.saxe@psy.ox.ac.uk.); Stephanie Nelli (Department of Experimental Psychology, University of Oxford, Oxford, UK. stephanie.nelli@psy.ox.ac.uk.); Christopher Summerfield (Department of Experimental Psychology, University of Oxford, Oxford, UK. christopher.summerfield@psy.ox.ac.uk.)",,Nature reviews. Neuroscience,10.1101/407007,33199854,,"{'text': 'Neuroscience research is undergoing a minor revolution. Recent advances in machine learning and artificial intelligence research have opened up new ways of thinking about neural computation. Many researchers are excited by the possibility that deep neural networks may offer theories of perception, cognition and action for biological brains. This approach has the potential to radically reshape our approach to understanding neural systems, because the computations performed by deep networks are learned from experience, and not endowed by the researcher. If so, how can neuroscientists use deep networks to model and understand biological brains? What is the outlook for neuroscientists who seek to characterize computations or neural codes, or who wish to understand perception, attention, memory and executive functions? In this Perspective, our goal is to offer a road map for systems neuroscience research in the age of deep learning. We discuss the conceptual and methodological challenges of comparing behaviour, learning dynamics and neural representations in artificial and biological systems, and we highlight new research questions that have emerged for neuroscience as a direct consequence of recent advances in machine learning.', 'structured': []}",0,pubmed
Graph Neural Networks with Coarse- and Fine-Grained Division for mitigating label noise and sparsity.,"Shuangjie Li (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; Department of Computer Science and Technology, Nanjing University, Nanjing, 210023, China. Electronic address: shuangjieli@smail.nju.edu.cn.); Baoming Zhang (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; Department of Computer Science and Technology, Nanjing University, Nanjing, 210023, China.); Jianqing Song (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; Department of Computer Science and Technology, Nanjing University, Nanjing, 210023, China.); Gaoli Ruan (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; Department of Computer Science and Technology, Nanjing University, Nanjing, 210023, China.); Chongjun Wang (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; Department of Computer Science and Technology, Nanjing University, Nanjing, 210023, China. Electronic address: chjwang@nju.edu.cn.); Junyuan Xie (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; Department of Computer Science and Technology, Nanjing University, Nanjing, 210023, China.)",,Neural networks : the official journal of the International Neural Network Society,10.1016/j.neunet.2025.107338,40086132,,"{'text': 'Graph Neural Networks (GNNs) have gained considerable prominence in semi-supervised learning tasks in processing graph-structured data, primarily owing to their message-passing mechanism, which largely relies on the availability of clean labels. However, in real-world scenarios, labels on nodes of graphs are inevitably noisy and sparsely labeled, significantly degrading the performance of GNNs. Exploring robust GNNs for semi-supervised node classification in the presence of noisy and sparse labels remains a critical challenge. Therefore, we propose a novel Graph Neural Network with Coarse- and Fine-Grained Division for mitigating label sparsity and noise, namely GNN-CFGD. The key idea of GNN-CFGD is reducing the negative impact of noisy labels via coarse- and fine-grained division, along with graph reconstruction. Specifically, we first investigate the effectiveness of linking unlabeled nodes to cleanly labeled nodes, demonstrating that this approach is more effective in combating labeling noise than linking to potentially noisy labeled nodes. Based on this observation, we introduce a Gaussian Mixture Model (GMM) based on the memory effect to perform a coarse-grained division of the given labels into clean and noisy labels. Next, we propose a clean labels oriented link that connects unlabeled nodes to cleanly labeled nodes, aimed at mitigating label sparsity and promoting supervision propagation. Furthermore, to provide refined supervision for noisy labeled nodes and additional supervision for unlabeled nodes, we fine-grain the noisy labeled and unlabeled nodes into two candidate sets based on confidence, respectively. Extensive experiments on various datasets demonstrate the superior effectiveness and robustness of GNN-CFGD.', 'structured': []}",0,pubmed
Vessel trajectory classification via transfer learning with Deep Convolutional Neural Networks.,"Hwan Kim (Department of Computer Science and Engineering, Chungnam National University, Daejeon, Korea.); Mingyu Choi (Department of Computer Science and Engineering, Chungnam National University, Daejeon, Korea.); Sekil Park (Korea Research Institude of Ships & Ocean Engineering (KRISO), Daejeon, Korea.); Sungsu Lim (Department of Computer Science and Engineering, Chungnam National University, Daejeon, Korea.)",,PloS one,10.1007/s11804-021-00228-9,39186723,,"{'text': ""The classification of vessel trajectories using Automatic Identification System (AIS) data is crucial for ensuring maritime safety and the efficient navigation of ships. The advent of deep learning has brought about more effective classification methods, utilizing Convolutional Neural Networks (CNN). However, existing CNN-based approaches primarily focus on either sailing or loitering movement patterns and struggle to capture valuable features and subtle differences between these patterns from input images. In response to these limitations, we firstly introduce a novel framework, Dense121-VMC, based on Deep Convolutional Neural Networks (DCNN) with transfer learning for simultaneous extraction and classification of both sailing and loitering trajectories. Our approach efficiently performs in extracting significant features from input images and in identifying subtle differences in each vessel's trajectory. Additionally, transfer learning effectively reduces data requirements and addresses the issue of overfitting. Through extended experiments, we demonstrate the novelty of proposed Dense121-VMC framework, achieving notable contributions for vessel trajectory classification."", 'structured': []}",0,pubmed
Analog versus discrete neural networks.,"B DasGupta (Department of Computer Science, University of Waterloo, Ontario, Canada.); G Schnitger (N/A)",,Neural computation,10.1162/neco.1996.8.4.805,8624961,,"{'text': 'We show that neural networks with three-times continuously differentiable activation functions are capable of computing a certain family of n-bit boolean functions with two gates, whereas networks composed of binary threshold functions require at least omega(log n) gates. Thus, for a large class of activation functions, analog neural networks can be more powerful than discrete neural networks, even when computing Boolean functions.', 'structured': []}",0,pubmed
Optimization of chemical libraries by neural networks.,"J Sadowski (ZHF/G - A 30, BASF Aktiengesellschaft, Ludwigshafen, D-67056, Germany. Sadowski@zhs4.zh.basf-ag.de)",,Current opinion in chemical biology,10.1016/s1367-5931(00)00089-2,10826977,,"{'text': 'Neural networks are finding ever-more applications in the design of combinatorial libraries. These can be divided into two types: Kohonen (self-organizing) maps, and feed-forward networks. While the number of applications is currently quite limited, a rapid increase in publications in this area can be expected in the next few years from the rapid development of general combinatorial chemistry technology.', 'structured': []}",0,pubmed
L1 -Norm Batch Normalization for Efficient Training of Deep Neural Networks.,Shuang Wu (N/A); Guoqi Li (N/A); Lei Deng (N/A); Liu Liu (N/A); Dong Wu (N/A); Yuan Xie (N/A); Luping Shi (N/A),,IEEE transactions on neural networks and learning systems,10.1109/TNNLS.2018.2876179,30418924,,"{'text': 'Batch normalization (BN) has recently become a standard component for accelerating and improving the training of deep neural networks (DNNs). However, BN brings in additional calculations, consumes more memory, and significantly slows down the training iteration. Furthermore, the nonlinear square and sqrt operations in the normalization process impede low bit-width quantization techniques, which draw much attention to the deep learning hardware community. In this paper, we propose an L1 -norm BN (L1BN) with only linear operations in both forward and backward propagations during training. L1BN is approximately equivalent to the conventional L2 -norm BN (L2BN) by multiplying a scaling factor that equals (π/2)', 'structured': []}",0,pubmed
Lecture Notes: Neural Network Architectures,Evelyn Herberg (N/A),2023,arXiv,,,2304.05133v2,"These lecture notes provide an overview of Neural Network architectures from
a mathematical point of view. Especially, Machine Learning with Neural Networks
is seen as an optimization problem. Covered are an introduction to Neural
Networks and the following architectures: Feedforward Neural Network,
Convolutional Neural Network, ResNet, and Recurrent Neural Network.",0,arxiv
Self-Organizing Multilayered Neural Networks of Optimal Complexity,V. Schetinin (N/A),2005,arXiv,,,cs/0504056v1,"The principles of self-organizing the neural networks of optimal complexity
is considered under the unrepresentative learning set. The method of
self-organizing the multi-layered neural networks is offered and used to train
the logical neural networks which were applied to the medical diagnostics.",0,arxiv
Neural Network Processing Neural Networks: An efficient way to learn higher order functions,Firat Tuna (N/A),2019,arXiv,,,1911.05640v2,"Functions are rich in meaning and can be interpreted in a variety of ways.
Neural networks were proven to be capable of approximating a large class of
functions[1]. In this paper, we propose a new class of neural networks called
""Neural Network Processing Neural Networks"" (NNPNNs), which inputs neural
networks and numerical values, instead of just numerical values. Thus enabling
neural networks to represent and process rich structures.",0,arxiv
Guaranteed Quantization Error Computation for Neural Network Model Compression,Wesley Cooke (N/A); Zihao Mo (N/A); Weiming Xiang (N/A),2023,arXiv,,,2304.13812v1,"Neural network model compression techniques can address the computation issue
of deep neural networks on embedded devices in industrial systems. The
guaranteed output error computation problem for neural network compression with
quantization is addressed in this paper. A merged neural network is built from
a feedforward neural network and its quantized version to produce the exact
output difference between two neural networks. Then, optimization-based methods
and reachability analysis methods are applied to the merged neural network to
compute the guaranteed quantization error. Finally, a numerical example is
proposed to validate the applicability and effectiveness of the proposed
approach.",0,arxiv
Graph Structure of Neural Networks,Jiaxuan You (N/A); Jure Leskovec (N/A); Kaiming He (N/A); Saining Xie (N/A),2020,arXiv,,,2007.06559v2,"Neural networks are often represented as graphs of connections between
neurons. However, despite their wide use, there is currently little
understanding of the relationship between the graph structure of the neural
network and its predictive performance. Here we systematically investigate how
does the graph structure of neural networks affect their predictive
performance. To this end, we develop a novel graph-based representation of
neural networks called relational graph, where layers of neural network
computation correspond to rounds of message exchange along the graph structure.
Using this representation we show that: (1) a ""sweet spot"" of relational graphs
leads to neural networks with significantly improved predictive performance;
(2) neural network's performance is approximately a smooth function of the
clustering coefficient and average path length of its relational graph; (3) our
findings are consistent across many different tasks and datasets; (4) the sweet
spot can be identified efficiently; (5) top-performing neural networks have
graph structure surprisingly similar to those of real biological neural
networks. Our work opens new directions for the design of neural architectures
and the understanding on neural networks in general.",0,arxiv
